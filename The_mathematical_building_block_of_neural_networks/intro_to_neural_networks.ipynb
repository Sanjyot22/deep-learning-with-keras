{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A FIRST LOOK AT A NEURAL NETWORK\n",
    "![title](./pics/neural-net.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "# keras imports\n",
    "from keras.utils import to_categorical\n",
    "from keras.datasets import mnist\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "# general imports\n",
    "from IPython.display import display, Markdown #just to display markdown\n",
    "import numpy as np\n",
    "\n",
    "# visualiaztion imports\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalars (0D tensors)\n",
    "A tensor that contains only one number is called a \"scalar\" (or \"scalar tensor\", or 0-dimensional tensor, or 0D tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array:12 , dimension of array:0\n"
     ]
    }
   ],
   "source": [
    "x = np.array(12)\n",
    "print (\"array:{} , dimension of array:{}\".format(x,x.ndim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectors (1D tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array:[12  3  6 14] , dimension of array:1\n"
     ]
    }
   ],
   "source": [
    "x = np.array([12, 3, 6, 14])\n",
    "print (\"array:{} , dimension of array:{}\".format(x,x.ndim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrices (2D tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array:[[ 5 78  2 34  0]\n",
      " [ 6 79  3 35  1]\n",
      " [ 7 80  4 36  2]] , dimension of array:2\n"
     ]
    }
   ],
   "source": [
    "x = np.array([  [5, 78, 2, 34, 0],\n",
    "                [6, 79, 3, 35, 1],\n",
    "                [7, 80, 4, 36, 2]])\n",
    "print (\"array:{} , dimension of array:{}\".format(x,x.ndim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D tensors and higher-dimensional tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array:[[[ 5 78  2 34  0]\n",
      "  [ 6 79  3 35  1]\n",
      "  [ 7 80  4 36  2]]\n",
      "\n",
      " [[ 5 78  2 34  0]\n",
      "  [ 6 79  3 35  1]\n",
      "  [ 7 80  4 36  2]]\n",
      "\n",
      " [[ 5 78  2 34  0]\n",
      "  [ 6 79  3 35  1]\n",
      "  [ 7 80  4 36  2]]] , dimension of array:3\n"
     ]
    }
   ],
   "source": [
    "x = np.array([\n",
    "    [\n",
    "        [5, 78, 2, 34, 0],\n",
    "        [6, 79, 3, 35, 1],\n",
    "        [7, 80, 4, 36, 2]\n",
    "    ],\n",
    "    [\n",
    "        [5, 78, 2, 34, 0],\n",
    "        [6, 79, 3, 35, 1],\n",
    "        [7, 80, 4, 36, 2]\n",
    "    ],\n",
    "    [\n",
    "        [5, 78, 2, 34, 0],\n",
    "        [6, 79, 3, 35, 1],\n",
    "        [7, 80, 4, 36, 2]\n",
    "    ]\n",
    "])\n",
    "print (\"array:{} , dimension of array:{}\".format(x,x.ndim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### data statistics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnist images are 28*28 images\n",
      "\n",
      "input_train shape: (60000, 784)\n",
      "input_test shape: (10000, 784) \n",
      "\n",
      "train samples shape: (60000, 10)\n",
      "test samples shape: (10000, 784) \n",
      "\n",
      "\n",
      "Imdb review data lables\n",
      "\n",
      "sample training lables\n",
      "[5 0 4 1 9 2 1 3 1 4]\n",
      "\n",
      "training lables after one hot encoding\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "displayign an images in training data\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# normalizing training and test images\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "# Preparing the labels\n",
    "temp_train_labels = train_labels[:10]\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "display(Markdown(\"### data statistics\")) \n",
    "print (\"mnist images are 28*28 images\\n\")\n",
    "print('input_train shape:', train_images.shape)\n",
    "print('input_test shape:', test_images.shape, \"\\n\")\n",
    "\n",
    "print('train samples shape:', train_labels.shape)\n",
    "print('test samples shape:', test_images.shape, \"\\n\")\n",
    "\n",
    "\n",
    "print(\"\\nImdb review data lables\\n\")\n",
    "\n",
    "\n",
    "print (\"sample training lables\")\n",
    "print (temp_train_labels)\n",
    "print (\"\\ntraining lables after one hot encoding\")\n",
    "print(train_labels[:10])\n",
    "\n",
    "print (\"\\ndisplayign an images in training data\")\n",
    "digit = train_images[4]\n",
    "plt.imshow(np.reshape(digit,(28,28)), cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The network architecture\n",
    "#### output = relu ( dot ( W, input ) + b )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "network.add(layers.Dense(10, activation='softmax'))\n",
    "network.summary()\n",
    "#The compilation step\n",
    "network.compile(optimizer='rmsprop',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.2566 - acc: 0.9256\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 0.1035 - acc: 0.9700\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.0674 - acc: 0.9801\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 0.0498 - acc: 0.9849\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 0.0368 - acc: 0.9890\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb4197b9b0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.fit(train_images, train_labels, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 34us/step\n",
      "\n",
      "test_acc: 0.98 %\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = network.evaluate(test_images, test_labels)\n",
    "print('\\ntest_acc:', round(test_acc,2),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A naive implementation of an element-wise \"relu\" operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 5, 78,  2, 34,  0],\n",
       "        [ 6, 79,  3, 35,  1],\n",
       "        [ 7, 80,  4, 36,  2]],\n",
       "\n",
       "       [[ 5, 78,  2, 34,  0],\n",
       "        [ 6, 79,  3, 35,  1],\n",
       "        [ 7, 80,  4, 36,  2]],\n",
       "\n",
       "       [[ 5, 78,  2, 34,  0],\n",
       "        [ 6, 79,  3, 35,  1],\n",
       "        [ 7, 80,  4, 36,  2]]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def naive_relu(x):\n",
    "    # x is 2D Numpy tensor\n",
    "    assert len(x.shape) == 2\n",
    "    x = x.copy()  # Avoid overwriting the input tensor\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i, j] = max(x[i, j], 0)\n",
    "return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A naive implementation of element-wise addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 5, 78,  2, 34,  0],\n",
       "        [ 6, 79,  3, 35,  1],\n",
       "        [ 7, 80,  4, 36,  2]],\n",
       "\n",
       "       [[ 5, 78,  2, 34,  0],\n",
       "        [ 6, 79,  3, 35,  1],\n",
       "        [ 7, 80,  4, 36,  2]],\n",
       "\n",
       "       [[ 5, 78,  2, 34,  0],\n",
       "        [ 6, 79,  3, 35,  1],\n",
       "        [ 7, 80,  4, 36,  2]]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def naive_add(x, y):\n",
    "    # x and y are 2D Numpy tensors\n",
    "    assert len(x.shape) == 2\n",
    "    assert x.shape == y.shape\n",
    "    x = x.copy()  # Avoid overwriting the input tensor\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i, j] += y[i, j]\n",
    "return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Native element-wise operation in Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Element-wise addition \n",
    "z = x + x\n",
    "# Element-wise relu\n",
    "z = np.maximum(z, 0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A naive implementation of matrix-vector addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 5, 78,  2, 34,  0],\n",
       "        [ 6, 79,  3, 35,  1],\n",
       "        [ 7, 80,  4, 36,  2]],\n",
       "\n",
       "       [[ 5, 78,  2, 34,  0],\n",
       "        [ 6, 79,  3, 35,  1],\n",
       "        [ 7, 80,  4, 36,  2]],\n",
       "\n",
       "       [[ 5, 78,  2, 34,  0],\n",
       "        [ 6, 79,  3, 35,  1],\n",
       "        [ 7, 80,  4, 36,  2]]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def naive_add_matrix_and_vector(x, y):\n",
    "    # x is a 2D Numpy tensor\n",
    "    # y is a Numpy vector\n",
    "    assert len(x.shape) == 2\n",
    "    assert len(y.shape) == 1\n",
    "    assert x.shape[1] == y.shape[0]\n",
    "    x = x.copy()  # Avoid overwriting the input tensor\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i, j] += y[j]\n",
    "return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the element-wise maximum operation to two tensors of different shapes via broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x is a random tensor with shape (64, 3, 32, 10)\n",
    "x = np.random.random((64, 3, 32, 10))\n",
    "# y is a random tensor with shape (32, 10)\n",
    "y = np.random.random((32, 10))\n",
    "# The output z has shape (64, 3, 32, 10) like x\n",
    "z = np.maximum(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A naive implementation of dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_vector_dot(x, y):\n",
    "    # x and y are Numpy vectors\n",
    "    assert len(x.shape) == 1\n",
    "    assert len(y.shape) == 1\n",
    "    assert x.shape[0] == y.shape[0]\n",
    "    z = 0.\n",
    "    for i in range(x.shape[0]):\n",
    "        z += x[i] * y[i]\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A naive implementation of matrix-vector dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_matrix_vector_dot(x, y):\n",
    "    # x is a Numpy matrix\n",
    "    # y is a Numpy vector\n",
    "    assert len(x.shape) == 2\n",
    "    assert len(y.shape) == 1\n",
    "    # The 1st dimension of x must be\n",
    "    # the same as the 0th dimension of y!\n",
    "    assert x.shape[1] == y.shape[0]\n",
    "    # This operation returns a vector of 0s\n",
    "    # with the same shape as y\n",
    "    z = np.zeros(x.shape[0])\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            z[i] += x[i, j] * y[j]\n",
    "    return z\n",
    "\n",
    "# Alternative naive implementation of matrix-vector dot\n",
    "\n",
    "def naive_matrix_vector_dot(x, y):\n",
    "    z = np.zeros(x.shape[0])\n",
    "    for i in range(x.shape[0]):\n",
    "        z[i] = naive_vector_dot(x[i, :], y)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A naive implementation of matrix-matrix dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.98293942, 0.31075006, 0.55330182, ..., 0.5676093 ,\n",
       "          0.39397712, 0.68619081],\n",
       "         [0.6369899 , 0.72789958, 0.80677869, ..., 0.28228535,\n",
       "          0.60666528, 0.90641031],\n",
       "         [0.72763897, 0.79358939, 0.46293096, ..., 0.90240892,\n",
       "          0.5279652 , 0.4384237 ],\n",
       "         ...,\n",
       "         [0.73875995, 0.52764322, 0.62711714, ..., 0.76746029,\n",
       "          0.31934214, 0.51313282],\n",
       "         [0.25556195, 0.385376  , 0.38339241, ..., 0.97190663,\n",
       "          0.72736785, 0.40879319],\n",
       "         [0.34899817, 0.77170657, 0.57441422, ..., 0.82831316,\n",
       "          0.96187731, 0.94454582]],\n",
       "\n",
       "        [[0.98293942, 0.11753079, 0.93337752, ..., 0.84821808,\n",
       "          0.63772758, 0.43428784],\n",
       "         [0.92125746, 0.71780968, 0.6669931 , ..., 0.28228535,\n",
       "          0.60666528, 0.90641031],\n",
       "         [0.75460514, 0.79358939, 0.52785136, ..., 0.90240892,\n",
       "          0.6341594 , 0.43116416],\n",
       "         ...,\n",
       "         [0.09566471, 0.52764322, 0.58063032, ..., 0.97705961,\n",
       "          0.85826878, 0.52491249],\n",
       "         [0.92586637, 0.385376  , 0.96628624, ..., 0.23370604,\n",
       "          0.51503349, 0.77202356],\n",
       "         [0.69064484, 0.3062873 , 0.51960978, ..., 0.82831316,\n",
       "          0.96187731, 0.89627737]],\n",
       "\n",
       "        [[0.98293942, 0.75145348, 0.69806764, ..., 0.79232405,\n",
       "          0.39397712, 0.84820255],\n",
       "         [0.30440146, 0.71780968, 0.78764437, ..., 0.68561323,\n",
       "          0.73315114, 0.90661343],\n",
       "         [0.50095763, 0.79358939, 0.5263913 , ..., 0.90240892,\n",
       "          0.8592031 , 0.45117075],\n",
       "         ...,\n",
       "         [0.67941985, 0.52764322, 0.58063032, ..., 0.76746029,\n",
       "          0.43628429, 0.51313282],\n",
       "         [0.90595696, 0.51322573, 0.5278414 , ..., 0.49301146,\n",
       "          0.15059316, 0.96434844],\n",
       "         [0.31168266, 0.84852282, 0.21036251, ..., 0.82831316,\n",
       "          0.96187731, 0.89627737]]],\n",
       "\n",
       "\n",
       "       [[[0.98293942, 0.16391241, 0.35531501, ..., 0.8245161 ,\n",
       "          0.54447264, 0.64558764],\n",
       "         [0.59248489, 0.71780968, 0.77430057, ..., 0.28228535,\n",
       "          0.60666528, 0.90641031],\n",
       "         [0.73761901, 0.79358939, 0.46293096, ..., 0.90240892,\n",
       "          0.5279652 , 0.93171567],\n",
       "         ...,\n",
       "         [0.51597732, 0.67184398, 0.58063032, ..., 0.76746029,\n",
       "          0.90957205, 0.51313282],\n",
       "         [0.18497154, 0.385376  , 0.40206981, ..., 0.4652545 ,\n",
       "          0.14083893, 0.15585048],\n",
       "         [0.58735067, 0.92752564, 0.66347711, ..., 0.82831316,\n",
       "          0.96187731, 0.89627737]],\n",
       "\n",
       "        [[0.98293942, 0.94083098, 0.5049469 , ..., 0.5676093 ,\n",
       "          0.39397712, 0.43428784],\n",
       "         [0.55839791, 0.71780968, 0.6669931 , ..., 0.78210547,\n",
       "          0.77888906, 0.90641031],\n",
       "         [0.50095763, 0.79358939, 0.74576132, ..., 0.90240892,\n",
       "          0.5279652 , 0.92798396],\n",
       "         ...,\n",
       "         [0.30704668, 0.52764322, 0.58063032, ..., 0.76746029,\n",
       "          0.48913839, 0.51313282],\n",
       "         [0.52996698, 0.99911276, 0.86246128, ..., 0.23370604,\n",
       "          0.99455943, 0.24008567],\n",
       "         [0.91634253, 0.46975319, 0.94946721, ..., 0.82831316,\n",
       "          0.96187731, 0.89627737]],\n",
       "\n",
       "        [[0.98293942, 0.25237991, 0.88386359, ..., 0.5676093 ,\n",
       "          0.60690918, 0.43428784],\n",
       "         [0.56951172, 0.99448048, 0.6669931 , ..., 0.44143979,\n",
       "          0.60666528, 0.94338942],\n",
       "         [0.96047527, 0.79358939, 0.46293096, ..., 0.90240892,\n",
       "          0.84215779, 0.21998809],\n",
       "         ...,\n",
       "         [0.16577472, 0.52764322, 0.58063032, ..., 0.76746029,\n",
       "          0.98946298, 0.51313282],\n",
       "         [0.15367078, 0.56525732, 0.38339241, ..., 0.67321171,\n",
       "          0.9626471 , 0.12497295],\n",
       "         [0.81976025, 0.90320971, 0.3590696 , ..., 0.82831316,\n",
       "          0.96187731, 0.89627737]]],\n",
       "\n",
       "\n",
       "       [[[0.98293942, 0.11753079, 0.35531501, ..., 0.5676093 ,\n",
       "          0.45189646, 0.43428784],\n",
       "         [0.30133884, 0.99353206, 0.6669931 , ..., 0.85913259,\n",
       "          0.60666528, 0.90641031],\n",
       "         [0.9382935 , 0.79358939, 0.59567804, ..., 0.90240892,\n",
       "          0.84874846, 0.98809002],\n",
       "         ...,\n",
       "         [0.8044574 , 0.52764322, 0.58063032, ..., 0.76746029,\n",
       "          0.1549457 , 0.51313282],\n",
       "         [0.59690616, 0.59638401, 0.38339241, ..., 0.66195616,\n",
       "          0.57331708, 0.2097258 ],\n",
       "         [0.73957976, 0.91621851, 0.18631738, ..., 0.82831316,\n",
       "          0.96187731, 0.89627737]],\n",
       "\n",
       "        [[0.98293942, 0.4750449 , 0.52111994, ..., 0.5676093 ,\n",
       "          0.39397712, 0.69293931],\n",
       "         [0.5030573 , 0.71780968, 0.92487808, ..., 0.28228535,\n",
       "          0.72301397, 0.90641031],\n",
       "         [0.50095763, 0.79358939, 0.46293096, ..., 0.90240892,\n",
       "          0.70583524, 0.8286707 ],\n",
       "         ...,\n",
       "         [0.04692668, 0.52764322, 0.68663089, ..., 0.99427996,\n",
       "          0.40087168, 0.51313282],\n",
       "         [0.61231538, 0.68132608, 0.74510071, ..., 0.45156445,\n",
       "          0.59347394, 0.45363128],\n",
       "         [0.72848321, 0.82520741, 0.76558093, ..., 0.88147621,\n",
       "          0.96187731, 0.89627737]],\n",
       "\n",
       "        [[0.98293942, 0.80456707, 0.99888593, ..., 0.91457047,\n",
       "          0.39397712, 0.93770359],\n",
       "         [0.30133884, 0.71780968, 0.6669931 , ..., 0.8290428 ,\n",
       "          0.60666528, 0.90641031],\n",
       "         [0.50095763, 0.79358939, 0.46293096, ..., 0.90240892,\n",
       "          0.5279652 , 0.96771073],\n",
       "         ...,\n",
       "         [0.46566544, 0.52764322, 0.58063032, ..., 0.87700246,\n",
       "          0.1549457 , 0.76575425],\n",
       "         [0.45700112, 0.385376  , 0.69455428, ..., 0.51633754,\n",
       "          0.22271324, 0.79482929],\n",
       "         [0.87931621, 0.69712833, 0.41939051, ..., 0.82831316,\n",
       "          0.96187731, 0.89627737]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[0.98293942, 0.66239023, 0.35531501, ..., 0.87733011,\n",
       "          0.39397712, 0.43428784],\n",
       "         [0.89139285, 0.71780968, 0.84898808, ..., 0.28228535,\n",
       "          0.60666528, 0.90641031],\n",
       "         [0.63682076, 0.79358939, 0.46293096, ..., 0.90240892,\n",
       "          0.73512596, 0.45968662],\n",
       "         ...,\n",
       "         [0.31261183, 0.52764322, 0.58063032, ..., 0.76746029,\n",
       "          0.62812045, 0.59018563],\n",
       "         [0.79469407, 0.385376  , 0.57155532, ..., 0.23370604,\n",
       "          0.57954717, 0.79140837],\n",
       "         [0.21787199, 0.38931402, 0.8241426 , ..., 0.82831316,\n",
       "          0.96187731, 0.89627737]],\n",
       "\n",
       "        [[0.98293942, 0.85727093, 0.57277155, ..., 0.5676093 ,\n",
       "          0.6817765 , 0.71448262],\n",
       "         [0.73922081, 0.71780968, 0.6669931 , ..., 0.68889028,\n",
       "          0.70512843, 0.90641031],\n",
       "         [0.50095763, 0.79358939, 0.46293096, ..., 0.93752781,\n",
       "          0.53897429, 0.94129148],\n",
       "         ...,\n",
       "         [0.83405978, 0.52764322, 0.58063032, ..., 0.76746029,\n",
       "          0.21902387, 0.51313282],\n",
       "         [0.6891297 , 0.86482074, 0.52971768, ..., 0.23370604,\n",
       "          0.68032846, 0.23929837],\n",
       "         [0.80479538, 0.96422684, 0.92439226, ..., 0.82831316,\n",
       "          0.96187731, 0.89627737]],\n",
       "\n",
       "        [[0.98293942, 0.9048887 , 0.35531501, ..., 0.5676093 ,\n",
       "          0.52153362, 0.45489382],\n",
       "         [0.71054266, 0.93963665, 0.6669931 , ..., 0.69029686,\n",
       "          0.97002645, 0.90641031],\n",
       "         [0.79872311, 0.79358939, 0.46293096, ..., 0.90240892,\n",
       "          0.5279652 , 0.27429331],\n",
       "         ...,\n",
       "         [0.13076221, 0.62886334, 0.58063032, ..., 0.8753254 ,\n",
       "          0.30754148, 0.51313282],\n",
       "         [0.74836779, 0.385376  , 0.79933953, ..., 0.23370604,\n",
       "          0.04195139, 0.11277925],\n",
       "         [0.369347  , 0.5417187 , 0.21843185, ..., 0.82831316,\n",
       "          0.96187731, 0.89627737]]],\n",
       "\n",
       "\n",
       "       [[[0.98293942, 0.81971116, 0.35531501, ..., 0.5676093 ,\n",
       "          0.87481795, 0.99550806],\n",
       "         [0.7783073 , 0.73798436, 0.90428721, ..., 0.3880636 ,\n",
       "          0.80616908, 0.9829598 ],\n",
       "         [0.59737572, 0.79358939, 0.46293096, ..., 0.90240892,\n",
       "          0.5279652 , 0.62198301],\n",
       "         ...,\n",
       "         [0.14325176, 0.52764322, 0.80782375, ..., 0.76746029,\n",
       "          0.86543099, 0.51313282],\n",
       "         [0.75047945, 0.385376  , 0.41826567, ..., 0.72759606,\n",
       "          0.28788669, 0.53712935],\n",
       "         [0.31721161, 0.64045937, 0.72759521, ..., 0.82831316,\n",
       "          0.96187731, 0.89627737]],\n",
       "\n",
       "        [[0.98293942, 0.11753079, 0.35531501, ..., 0.5676093 ,\n",
       "          0.72128639, 0.43428784],\n",
       "         [0.68211408, 0.88451687, 0.6669931 , ..., 0.3590372 ,\n",
       "          0.85874322, 0.90641031],\n",
       "         [0.50095763, 0.79358939, 0.50525161, ..., 0.97615782,\n",
       "          0.93309558, 0.13284287],\n",
       "         ...,\n",
       "         [0.20393488, 0.52764322, 0.58063032, ..., 0.76746029,\n",
       "          0.56424204, 0.51313282],\n",
       "         [0.75202588, 0.61870349, 0.38339241, ..., 0.25869693,\n",
       "          0.48497097, 0.58119485],\n",
       "         [0.51906824, 0.28164792, 0.64451259, ..., 0.82831316,\n",
       "          0.96187731, 0.89627737]],\n",
       "\n",
       "        [[0.98293942, 0.98669812, 0.44914682, ..., 0.87643161,\n",
       "          0.44578897, 0.51729577],\n",
       "         [0.30133884, 0.71780968, 0.6669931 , ..., 0.28761915,\n",
       "          0.60666528, 0.90641031],\n",
       "         [0.50095763, 0.88338967, 0.46293096, ..., 0.90240892,\n",
       "          0.65777986, 0.30288061],\n",
       "         ...,\n",
       "         [0.43406239, 0.52764322, 0.58063032, ..., 0.98916033,\n",
       "          0.74678213, 0.51313282],\n",
       "         [0.31907152, 0.93248687, 0.8808844 , ..., 0.86664803,\n",
       "          0.46645852, 0.07250081],\n",
       "         [0.714533  , 0.52649585, 0.9700598 , ..., 0.82831316,\n",
       "          0.96187731, 0.89627737]]],\n",
       "\n",
       "\n",
       "       [[[0.98293942, 0.92444582, 0.35531501, ..., 0.91814322,\n",
       "          0.39397712, 0.43428784],\n",
       "         [0.34388052, 0.71780968, 0.6669931 , ..., 0.84679689,\n",
       "          0.60666528, 0.90641031],\n",
       "         [0.50095763, 0.79358939, 0.91168909, ..., 0.90240892,\n",
       "          0.5279652 , 0.50337253],\n",
       "         ...,\n",
       "         [0.57247583, 0.93232847, 0.60520539, ..., 0.97237818,\n",
       "          0.93963129, 0.87449562],\n",
       "         [0.50626943, 0.385376  , 0.8141273 , ..., 0.95239006,\n",
       "          0.78275144, 0.98690753],\n",
       "         [0.14853568, 0.62676842, 0.23420414, ..., 0.82831316,\n",
       "          0.96187731, 0.89627737]],\n",
       "\n",
       "        [[0.98293942, 0.63916462, 0.73234457, ..., 0.96985504,\n",
       "          0.40643263, 0.78752482],\n",
       "         [0.30133884, 0.71780968, 0.6669931 , ..., 0.59213249,\n",
       "          0.8321134 , 0.90641031],\n",
       "         [0.52851749, 0.79358939, 0.46293096, ..., 0.90240892,\n",
       "          0.97776158, 0.09797672],\n",
       "         ...,\n",
       "         [0.70155076, 0.87348292, 0.68041011, ..., 0.76746029,\n",
       "          0.94609019, 0.51313282],\n",
       "         [0.37433736, 0.385376  , 0.38339241, ..., 0.75914028,\n",
       "          0.37561139, 0.86800276],\n",
       "         [0.34272235, 0.91871912, 0.81093731, ..., 0.82831316,\n",
       "          0.96187731, 0.89627737]],\n",
       "\n",
       "        [[0.98293942, 0.45338075, 0.56669632, ..., 0.5676093 ,\n",
       "          0.44549297, 0.5898018 ],\n",
       "         [0.74986048, 0.71780968, 0.6669931 , ..., 0.61282402,\n",
       "          0.60666528, 0.95256307],\n",
       "         [0.50095763, 0.79358939, 0.46293096, ..., 0.90240892,\n",
       "          0.9674847 , 0.78829527],\n",
       "         ...,\n",
       "         [0.80371813, 0.78753028, 0.73455901, ..., 0.76746029,\n",
       "          0.22888743, 0.51313282],\n",
       "         [0.24951107, 0.385376  , 0.38339241, ..., 0.45035203,\n",
       "          0.04195139, 0.17136369],\n",
       "         [0.22711734, 0.28164792, 0.69411419, ..., 0.82831316,\n",
       "          0.96187731, 0.89627737]]]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def naive_matrix_dot(x, y):\n",
    "    # x and y are Numpy matrices\n",
    "    assert len(x.shape) == 2\n",
    "    assert len(y.shape) == 2\n",
    "    # The 1st dimension of x must be\n",
    "    # the same as the 0th dimension of y!\n",
    "    assert x.shape[1] == y.shape[0]\n",
    "    # This operation returns a matrix of 0s\n",
    "    # with a specific shape\n",
    "    z = np.zeros((x.shape[0], y.shape[1]))\n",
    "    # We iterate over the rows of x\n",
    "    for i in range(x.shape[0]):\n",
    "        # And over the columns of y\n",
    "        for j in range(y.shape[1]):\n",
    "            row_x = x[i, :]\n",
    "            column_y = y[:, j]\n",
    "            z[i, j] = naive_vector_dot(row_x, column_y)\n",
    "return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What’s a derivative?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a continuous, smooth function \n",
    "\n",
    "```f(x) = y,``` \n",
    "\n",
    "mapping a real number x to a new real number y. \n",
    "Because the function is continuous, a small change in x can only result in a small change in y—that’s the intuition behind continuity. Let’s say you increase x by a small factor epsilon_x: this results in an small epsilon_y change to y.\n",
    "\n",
    "```f(x + epsilon_x) = y + epsilon_y```\n",
    "\n",
    "when epsilon_x is \"small enough\", then around a certain point p, it is possible to approximate f as a linear function of slope a, so that epsilon_y becomes a * epsilon_x:\n",
    "\n",
    "\n",
    "```f(x + epsilon_x) = y + a * epsilon_x```\n",
    "\n",
    "![title](./pics/derivative.png)\n",
    "\n",
    "We saw earlier that the derivative of a function f(x) of a single coefficient could be interpreted as the slope of the curve of f. Likewise, gradient(f)(W0) can be interpreted as the tensor describing the curvature of f(W) around W0\n",
    "\n",
    "For this reason, in much the same way that, for a function f(x), you could lower the value of f(x) by moving x by a little bit in the direction opposite to the derivative, with a function f(W) of a tensor, you can lower f(W) by moving W in the direction opposite to the gradient, \n",
    "\n",
    "\n",
    "e.g. ```W1 = W0 - step * gradient(f)(W0)``` \n",
    "(where step is a small scaling factor). \n",
    "\n",
    "\n",
    "That simply means \"going opposite to the curvature\", which intuitively should get you lower on the curve. Note that the scaling factor step is needed because gradient(f)(W0) only approximates the curvature when you are close to W0, so you don’t want to get too far away from W0\n",
    "\n",
    "![title](./pics/sgd.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A naive implementation of gradient descent with momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PSEUDO CODE\n",
    "past_velocity = 0.\n",
    "momentum = 0.1  # A constant momentum factor\n",
    "while loss > 0.01:  # Optimization loop\n",
    "    w, loss, gradient = get_current_parameters()\n",
    "    velocity = past_velocity * momentum + learning_rate * gradient\n",
    "    w = w + momentum * velocity - learning_rate * gradient\n",
    "    past_velocity = velocity\n",
    "    update_parameter(w)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANATOMY OF A NEURAL NETWORK\n",
    "![title](./pics/neural-net.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "\n",
    "# keras imports\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "\n",
    "# general imports\n",
    "import numpy as np\n",
    "from IPython.display import display, Markdown #just to display markdown\n",
    "\n",
    "# utility functions\n",
    "import os\n",
    "import sys\n",
    "%matplotlib inline\n",
    "os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(\"__file__\"))))\n",
    "from utility.utils import utils\n",
    "utility_obj = utils()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUTOMATIC SHAPE INFERENCE \n",
    " \n",
    "The layers that you add to keras models are dynamically built to match the shape of the incoming layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### After first layers' {None,32} dimension(None stands for number of examples in batch), the input shape of next layer is automatically interpreted"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 6)                 198       \n",
      "=================================================================\n",
      "Total params: 25,318\n",
      "Trainable params: 25,318\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "display(Markdown(\"#### After first layers' {None,32} dimension(None stands for number of examples in batch), the input shape of next layer is automatically interpreted\")) \n",
    "# A dense layer with 6 output units\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(32, input_shape=(784,)))\n",
    "model.add(layers.Dense(6))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In Keras models can be developed either by using Sequential model or Functional API's\n",
    "\n",
    "Lets build model below using both the metheodologies\n",
    "![title](./pics/model-architecture.png)\n",
    "\n",
    "\n",
    "## A network definition using the Sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_14 (Dense)             (None, 16)                160016    \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 160,305\n",
      "Trainable params: 160,305\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A network definition using the functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 10000)             0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                160016    \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 160,305\n",
      "Trainable params: 160,305\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_tensor = layers.Input(shape=(10000,))\n",
    "x = layers.Dense(16, activation='relu')(input_tensor)\n",
    "x = layers.Dense(16, activation='relu')(x)\n",
    "output_tensor = layers.Dense(1, activation='sigmoid')(x)\n",
    "model = models.Model(input_tensor, output_tensor)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a loss function and an optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "IMDB data encoding from text will be extensively covered in \"text_pre-processing_basic model_building\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "before data preprocessing\n",
      "input_train shape: (25000,)\n",
      "input_test shape: (25000,) \n",
      "\n",
      "\n",
      "Imdb review 1 sample input data\n",
      "\n",
      "[list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32])]\n",
      "\n",
      "Imdb review data lables\n",
      "[1 0]\n",
      "\n",
      "Note:  \n",
      "0 : \"Negative review\"\n",
      "1 : \"Positive review\"\n",
      "post_padding in input data helps gragient in LSTM flow better \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pre-processing initializations\n",
    "max_features = 10000  # number of words to consider as features\n",
    "maxlen = 500  # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32\n",
    "\n",
    "# data pre-prcessing\n",
    "### IMDB data encoding from text will be extensively covered in \"text_pre-processing_basic model_building\" ##\n",
    "\n",
    "print('Loading data...')\n",
    "(input_train, y_train), (input_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "print (\"before data preprocessing\")\n",
    "print('input_train shape:', input_train.shape)\n",
    "print('input_test shape:', input_test.shape, \"\\n\")\n",
    "\n",
    "print(\"\\nImdb review 1 sample input data\\n\")\n",
    "print(input_train[:1]) \n",
    "\n",
    "print(\"\\nImdb review data lables\")\n",
    "print(y_train[:2])\n",
    "print(\n",
    "\"\"\"\n",
    "Note:  \n",
    "0 : \"Negative review\"\n",
    "1 : \"Positive review\"\n",
    "post_padding in input data helps gragient in LSTM flow better \n",
    "\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are two ways we could do that:\n",
    "\n",
    "- We could pad our lists so that they all have the same length, and turn them into an integer tensor of shape (samples, word_indices), then use as first layer in our network a layer capable of handling such integer tensors (the Embedding layer, which we will cover in detail later in the book).\n",
    "\n",
    "- We could one-hot-encode our lists to turn them into vectors of 0s and 1s. Concretely, this would mean for instance turning the sequence [3, 5] into a 10,000-dimensional vector that would be all-zeros except for indices 3 and 5, which would be ones. Then we could use as first layer in our network a Dense layer, capable of handling floating point vector data.\n",
    "\n",
    "#### We will go with the latter solution\n",
    "\n",
    "## Encoding the integer sequences into a binary matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Sample input data after encoding\n",
      "\n",
      "[[0. 1. 1. ... 0. 0. 0.]]\n",
      "\n",
      "Imdb review data lables after encoding\n",
      "[[0. 1. 1. ... 0. 0. 0.]\n",
      " [0. 1. 1. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    # Create an all-zero matrix of shape (len(sequences), dimension)\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.  # set specific indices of results[i] to 1s\n",
    "    return results\n",
    "\n",
    "# Our vectorized training data\n",
    "x_train = vectorize_sequences(input_train)\n",
    "\n",
    "# Our vectorized test data\n",
    "x_test = vectorize_sequences(input_test)\n",
    "\n",
    "# Our vectorized labels\n",
    "y_train = np.asarray(y_train).astype('float32')\n",
    "y_test = np.asarray(y_test).astype('float32')\n",
    "\n",
    "print(\"\\n Sample input data after encoding\\n\")\n",
    "print(x_train[:1]) \n",
    "\n",
    "print(\"\\nImdb review data lables after encoding\")\n",
    "print(x_test[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting aside a validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_train[:10000]\n",
    "y_val = y_train[:10000]\n",
    "\n",
    "partial_x_train = x_train[10000:]\n",
    "partial_y_train = y_train[10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "15000/15000 [==============================] - 2s 105us/step - loss: 0.5049 - acc: 0.7873 - val_loss: 0.3723 - val_acc: 0.8739\n",
      "Epoch 2/20\n",
      "15000/15000 [==============================] - 1s 87us/step - loss: 0.2932 - acc: 0.9046 - val_loss: 0.3007 - val_acc: 0.8864\n",
      "Epoch 3/20\n",
      "15000/15000 [==============================] - 1s 88us/step - loss: 0.2131 - acc: 0.9299 - val_loss: 0.2818 - val_acc: 0.8880\n",
      "Epoch 4/20\n",
      "15000/15000 [==============================] - 1s 90us/step - loss: 0.1694 - acc: 0.9447 - val_loss: 0.2813 - val_acc: 0.8859\n",
      "Epoch 5/20\n",
      "15000/15000 [==============================] - 1s 86us/step - loss: 0.1352 - acc: 0.9565 - val_loss: 0.2867 - val_acc: 0.8841\n",
      "Epoch 6/20\n",
      "15000/15000 [==============================] - 1s 87us/step - loss: 0.1132 - acc: 0.9645 - val_loss: 0.3042 - val_acc: 0.8860\n",
      "Epoch 7/20\n",
      "15000/15000 [==============================] - 1s 91us/step - loss: 0.0935 - acc: 0.9729 - val_loss: 0.3199 - val_acc: 0.8807\n",
      "Epoch 8/20\n",
      "15000/15000 [==============================] - 1s 87us/step - loss: 0.0774 - acc: 0.9796 - val_loss: 0.3468 - val_acc: 0.8790\n",
      "Epoch 9/20\n",
      "15000/15000 [==============================] - 1s 89us/step - loss: 0.0644 - acc: 0.9831 - val_loss: 0.3787 - val_acc: 0.8732\n",
      "Epoch 10/20\n",
      "15000/15000 [==============================] - 1s 90us/step - loss: 0.0524 - acc: 0.9869 - val_loss: 0.4302 - val_acc: 0.8714\n",
      "Epoch 11/20\n",
      "15000/15000 [==============================] - 1s 88us/step - loss: 0.0415 - acc: 0.9911 - val_loss: 0.4267 - val_acc: 0.8760\n",
      "Epoch 12/20\n",
      "15000/15000 [==============================] - 1s 91us/step - loss: 0.0357 - acc: 0.9919 - val_loss: 0.4435 - val_acc: 0.8735\n",
      "Epoch 13/20\n",
      "15000/15000 [==============================] - 1s 90us/step - loss: 0.0301 - acc: 0.9931 - val_loss: 0.4709 - val_acc: 0.8734\n",
      "Epoch 14/20\n",
      "15000/15000 [==============================] - 1s 91us/step - loss: 0.0248 - acc: 0.9938 - val_loss: 0.4991 - val_acc: 0.8725\n",
      "Epoch 15/20\n",
      "15000/15000 [==============================] - 1s 85us/step - loss: 0.0139 - acc: 0.9987 - val_loss: 0.6205 - val_acc: 0.8607\n",
      "Epoch 16/20\n",
      "15000/15000 [==============================] - 1s 91us/step - loss: 0.0158 - acc: 0.9979 - val_loss: 0.5587 - val_acc: 0.8709\n",
      "Epoch 17/20\n",
      "15000/15000 [==============================] - 1s 90us/step - loss: 0.0105 - acc: 0.9987 - val_loss: 0.6728 - val_acc: 0.8629\n",
      "Epoch 18/20\n",
      "15000/15000 [==============================] - 1s 88us/step - loss: 0.0081 - acc: 0.9990 - val_loss: 0.6759 - val_acc: 0.8556\n",
      "Epoch 19/20\n",
      "15000/15000 [==============================] - 1s 92us/step - loss: 0.0049 - acc: 0.9999 - val_loss: 0.6931 - val_acc: 0.8665\n",
      "Epoch 20/20\n",
      "15000/15000 [==============================] - 1s 89us/step - loss: 0.0072 - acc: 0.9987 - val_loss: 0.6886 - val_acc: 0.8665\n"
     ]
    }
   ],
   "source": [
    " history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the training and validation loss & accuracy from model training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ploting model training results\n",
    "utility_obj.plot_training_history(history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating predictions for new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00581402],\n",
       "       [1.        ],\n",
       "       [0.990448  ],\n",
       "       ...,\n",
       "       [0.00429063],\n",
       "       [0.01205759],\n",
       "       [0.51759356]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a multi-class classification model on the reuters dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "# keras imports\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "from keras.datasets import reuters\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "\n",
    "# general imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Markdown #just to display markdown\n",
    "\n",
    "# utility functions\n",
    "import os\n",
    "import sys\n",
    "%matplotlib inline\n",
    "os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(\"__file__\"))))\n",
    "from utility.utils import utils\n",
    "utility_obj = utils()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "before data preprocessing\n",
      "input_train shape: (8982,)\n",
      "input_test shape: (2246,) \n",
      "\n",
      "\n",
      "reuters dataset's 1 sample input data\n",
      "\n",
      "[list([1, 2, 2, 8, 43, 10, 447, 5, 25, 207, 270, 5, 3095, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 4579, 1005, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 1245, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12])]\n",
      "\n",
      "reuters dataset's 2 data lables\n",
      "[3 4]\n",
      "\n",
      " reuters dataset's sample input data after encoding\n",
      "\n",
      "[[0. 1. 1. ... 0. 0. 0.]]\n",
      "\n",
      "reuters dataset's lables after encoding\n",
      "[[0. 1. 1. ... 0. 0. 0.]\n",
      " [0. 1. 1. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Loading the Reuters dataset\n",
    "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)\n",
    "\n",
    "# Taking a look at the data\n",
    "print (\"\\nbefore data preprocessing\")\n",
    "print('input_train shape:', train_data.shape)\n",
    "print('input_test shape:', test_data.shape, \"\\n\")\n",
    "\n",
    "print(\"\\nreuters dataset's 1 sample input data\\n\")\n",
    "print(train_data[:1]) \n",
    "\n",
    "print(\"\\nreuters dataset's 2 data lables\")\n",
    "print(train_labels[:2])\n",
    "\n",
    "# pre-processing the dataset\n",
    "\n",
    "# sentences pre-processing\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "# Our vectorized training data\n",
    "x_train = vectorize_sequences(train_data)\n",
    "# Our vectorized test data\n",
    "x_test = vectorize_sequences(test_data)\n",
    "\n",
    "print(\"\\n reuters dataset's sample input data after encoding\\n\")\n",
    "print(x_train[:1]) \n",
    "\n",
    "print(\"\\nreuters dataset's lables after encoding\")\n",
    "print(x_test[:2])\n",
    "\n",
    "# lables one-hot encoding\n",
    "def to_one_hot(labels, dimension=46):\n",
    "    results = np.zeros((len(labels), dimension))\n",
    "    for i, label in enumerate(labels):\n",
    "        results[i, label] = 1.\n",
    "    return results\n",
    "# Our vectorized training labels\n",
    "one_hot_train_labels = to_one_hot(train_labels)\n",
    "# Our vectorized test labels\n",
    "one_hot_test_labels = to_one_hot(test_labels)\n",
    "\n",
    "# keras one-liner (lables one-hot encoding)\n",
    "one_hot_train_labels = to_categorical(train_labels)\n",
    "one_hot_test_labels = to_categorical(test_labels)\n",
    "\n",
    "\n",
    "# model architecture development\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "# Compiling our model\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Setting aside a validation set\n",
    "x_val = x_train[:1000]\n",
    "partial_x_train = x_train[1000:]\n",
    "y_val = one_hot_train_labels[:1000]\n",
    "partial_y_train = one_hot_train_labels[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model on reuter dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/12\n",
      "7982/7982 [==============================] - 1s 94us/step - loss: 2.5322 - acc: 0.4955 - val_loss: 1.7208 - val_acc: 0.6120\n",
      "Epoch 2/12\n",
      "7982/7982 [==============================] - 1s 64us/step - loss: 1.4452 - acc: 0.6879 - val_loss: 1.3459 - val_acc: 0.7060\n",
      "Epoch 3/12\n",
      "7982/7982 [==============================] - 1s 63us/step - loss: 1.0953 - acc: 0.7651 - val_loss: 1.1708 - val_acc: 0.7430\n",
      "Epoch 4/12\n",
      "7982/7982 [==============================] - 1s 65us/step - loss: 0.8697 - acc: 0.8165 - val_loss: 1.0793 - val_acc: 0.7590\n",
      "Epoch 5/12\n",
      "7982/7982 [==============================] - 1s 67us/step - loss: 0.7034 - acc: 0.8472 - val_loss: 0.9844 - val_acc: 0.7810\n",
      "Epoch 6/12\n",
      "7982/7982 [==============================] - 1s 67us/step - loss: 0.5667 - acc: 0.8802 - val_loss: 0.9411 - val_acc: 0.8040\n",
      "Epoch 7/12\n",
      "7982/7982 [==============================] - 1s 67us/step - loss: 0.4581 - acc: 0.9048 - val_loss: 0.9083 - val_acc: 0.8020\n",
      "Epoch 8/12\n",
      "7982/7982 [==============================] - 1s 65us/step - loss: 0.3695 - acc: 0.9231 - val_loss: 0.9363 - val_acc: 0.7890\n",
      "Epoch 9/12\n",
      "7982/7982 [==============================] - 1s 64us/step - loss: 0.3032 - acc: 0.9315 - val_loss: 0.8917 - val_acc: 0.8090\n",
      "Epoch 10/12\n",
      "7982/7982 [==============================] - 1s 66us/step - loss: 0.2537 - acc: 0.9414 - val_loss: 0.9071 - val_acc: 0.8110\n",
      "Epoch 11/12\n",
      "7982/7982 [==============================] - 1s 66us/step - loss: 0.2187 - acc: 0.9471 - val_loss: 0.9177 - val_acc: 0.8130\n",
      "Epoch 12/12\n",
      "7982/7982 [==============================] - 1s 63us/step - loss: 0.1873 - acc: 0.9508 - val_loss: 0.9027 - val_acc: 0.8130\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=12,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the training and validation loss & accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Values captured in history: {}\".format(history_dict.keys()))\n",
    "# ploting model training results\n",
    "utility_obj.plot_training_history(history.history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

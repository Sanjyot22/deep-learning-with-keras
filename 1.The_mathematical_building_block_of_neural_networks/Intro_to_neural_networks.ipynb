{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A FIRST LOOK AT A NEURAL NETWORK\n",
    "![title](./pics/neural-net.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "# keras imports\n",
    "from keras.utils import to_categorical\n",
    "from keras.datasets import mnist\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "# general imports\n",
    "from IPython.display import display, Markdown #just to display markdown\n",
    "import numpy as np\n",
    "\n",
    "# utility functions\n",
    "import os\n",
    "import sys\n",
    "%matplotlib inline\n",
    "os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(\"__file__\"))))\n",
    "from utility.utils import utils\n",
    "utility_obj = utils()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scalars (0D tensors)\n",
    "A tensor that contains only one number is called a \"scalar\" (or \"scalar tensor\", or 0-dimensional tensor, or 0D tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(12)\n",
    "print (\"array:{} , dimension of array:{}\".format(x,x.ndim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectors (1D tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([12, 3, 6, 14])\n",
    "print (\"array:{} , dimension of array:{}\".format(x,x.ndim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrices (2D tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([  [5, 78, 2, 34, 0],\n",
    "                [6, 79, 3, 35, 1],\n",
    "                [7, 80, 4, 36, 2]])\n",
    "print (\"array:{} , dimension of array:{}\".format(x,x.ndim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D tensors and higher-dimensional tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([\n",
    "    [\n",
    "        [5, 78, 2, 34, 0],\n",
    "        [6, 79, 3, 35, 1],\n",
    "        [7, 80, 4, 36, 2]\n",
    "    ],\n",
    "    [\n",
    "        [5, 78, 2, 34, 0],\n",
    "        [6, 79, 3, 35, 1],\n",
    "        [7, 80, 4, 36, 2]\n",
    "    ],\n",
    "    [\n",
    "        [5, 78, 2, 34, 0],\n",
    "        [6, 79, 3, 35, 1],\n",
    "        [7, 80, 4, 36, 2]\n",
    "    ]\n",
    "])\n",
    "print (\"array:{} , dimension of array:{}\".format(x,x.ndim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing mnist data for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# normalizing training and test images\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "# Preparing the labels\n",
    "temp_train_labels = train_labels[:10]\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "display(Markdown(\"#### data statistics\")) \n",
    "print (\"mnist images are 28*28 images\\n\")\n",
    "print('input_train shape:', train_images.shape)\n",
    "print('input_test shape:', test_images.shape, \"\\n\")\n",
    "\n",
    "print('train samples shape:', train_labels.shape)\n",
    "print('test samples shape:', test_images.shape, \"\\n\")\n",
    "\n",
    "\n",
    "print(\"\\nImdb review data lables\\n\")\n",
    "\n",
    "\n",
    "print (\"sample training lables\")\n",
    "print (temp_train_labels)\n",
    "print (\"\\ntraining lables after one hot encoding\")\n",
    "print(train_labels[:10])\n",
    "\n",
    "print (\"\\ndisplayign an images in training data\")\n",
    "digit = train_images[4]\n",
    "plt.imshow(np.reshape(digit,(28,28)), cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The network architecture\n",
    "#### output = relu ( dot ( W, input ) + b )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "network.add(layers.Dense(10, activation='softmax'))\n",
    "network.summary()\n",
    "#The compilation step\n",
    "network.compile(optimizer='rmsprop',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = network.fit(train_images, train_labels, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ploting model training results\n",
    "utility_obj.plot_training_history(history_dict=history.history, plot_val=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = network.evaluate(test_images, test_labels)\n",
    "print('\\ntest_acc:', round(test_acc,2),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic naive functions implementations in numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A naive implementation of an element-wise \"relu\" operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_relu(x):\n",
    "    # x is 2D Numpy tensor\n",
    "    assert len(x.shape) == 2\n",
    "    x = x.copy()  # Avoid overwriting the input tensor\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i, j] = max(x[i, j], 0)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A naive implementation of element-wise addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_add(x, y):\n",
    "    # x and y are 2D Numpy tensors\n",
    "    assert len(x.shape) == 2\n",
    "    assert x.shape == y.shape\n",
    "    x = x.copy()  # Avoid overwriting the input tensor\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i, j] += y[i, j]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Native element-wise operation in Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Element-wise addition \n",
    "z = x + x\n",
    "# Element-wise relu\n",
    "z = np.maximum(z, 0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A naive implementation of matrix-vector addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_add_matrix_and_vector(x, y):\n",
    "    # x is a 2D Numpy tensor\n",
    "    # y is a Numpy vector\n",
    "    assert len(x.shape) == 2\n",
    "    assert len(y.shape) == 1\n",
    "    assert x.shape[1] == y.shape[0]\n",
    "    x = x.copy()  # Avoid overwriting the input tensor\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i, j] += y[j]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the element-wise maximum operation to two tensors of different shapes via broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x is a random tensor with shape (64, 3, 32, 10)\n",
    "x = np.random.random((64, 3, 32, 10))\n",
    "# y is a random tensor with shape (32, 10)\n",
    "y = np.random.random((32, 10))\n",
    "# The output z has shape (64, 3, 32, 10) like x\n",
    "z = np.maximum(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A naive implementation of dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_vector_dot(x, y):\n",
    "    # x and y are Numpy vectors\n",
    "    assert len(x.shape) == 1\n",
    "    assert len(y.shape) == 1\n",
    "    assert x.shape[0] == y.shape[0]\n",
    "    z = 0.\n",
    "    for i in range(x.shape[0]):\n",
    "        z += x[i] * y[i]\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A naive implementation of matrix-vector dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_matrix_vector_dot(x, y):\n",
    "    # x is a Numpy matrix\n",
    "    # y is a Numpy vector\n",
    "    assert len(x.shape) == 2\n",
    "    assert len(y.shape) == 1\n",
    "    # The 1st dimension of x must be\n",
    "    # the same as the 0th dimension of y!\n",
    "    assert x.shape[1] == y.shape[0]\n",
    "    # This operation returns a vector of 0s\n",
    "    # with the same shape as y\n",
    "    z = np.zeros(x.shape[0])\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            z[i] += x[i, j] * y[j]\n",
    "    return z\n",
    "\n",
    "# Alternative naive implementation of matrix-vector dot\n",
    "\n",
    "def naive_matrix_vector_dot(x, y):\n",
    "    z = np.zeros(x.shape[0])\n",
    "    for i in range(x.shape[0]):\n",
    "        z[i] = naive_vector_dot(x[i, :], y)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A naive implementation of matrix-matrix dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_matrix_dot(x, y):\n",
    "    # x and y are Numpy matrices\n",
    "    assert len(x.shape) == 2\n",
    "    assert len(y.shape) == 2\n",
    "    # The 1st dimension of x must be\n",
    "    # the same as the 0th dimension of y!\n",
    "    assert x.shape[1] == y.shape[0]\n",
    "    # This operation returns a matrix of 0s\n",
    "    # with a specific shape\n",
    "    z = np.zeros((x.shape[0], y.shape[1]))\n",
    "    # We iterate over the rows of x\n",
    "    for i in range(x.shape[0]):\n",
    "        # And over the columns of y\n",
    "        for j in range(y.shape[1]):\n",
    "            row_x = x[i, :]\n",
    "            column_y = y[:, j]\n",
    "            z[i, j] = naive_vector_dot(row_x, column_y)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A naive implementation of gradient descent with momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################   PSEUDO CODE   #############################\n",
    "past_velocity = 0.\n",
    "loss = 100\n",
    "momentum = 0.1  # A constant momentum factor\n",
    "while loss > 0.01:  # Optimization loop\n",
    "    w, loss, gradient = get_current_parameters()\n",
    "    velocity = past_velocity * momentum + learning_rate * gradient\n",
    "    w = w + momentum * velocity - learning_rate * gradient\n",
    "    past_velocity = velocity\n",
    "    update_parameter(w)\n",
    "####################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What’s a derivative?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a continuous, smooth function \n",
    "\n",
    "```f(x) = y,``` \n",
    "\n",
    "mapping a real number x to a new real number y. \n",
    "Because the function is continuous, a small change in x can only result in a small change in y—that’s the intuition behind continuity. Let’s say you increase x by a small factor epsilon_x: this results in an small epsilon_y change to y.\n",
    "\n",
    "```f(x + epsilon_x) = y + epsilon_y```\n",
    "\n",
    "when epsilon_x is \"small enough\", then around a certain point p, it is possible to approximate f as a linear function of slope a, so that epsilon_y becomes a * epsilon_x:\n",
    "\n",
    "\n",
    "```f(x + epsilon_x) = y + a * epsilon_x```\n",
    "\n",
    "![title](./pics/derivative.png)\n",
    "\n",
    "We saw earlier that the derivative of a function f(x) of a single coefficient could be interpreted as the slope of the curve of f. Likewise, gradient(f)(W0) can be interpreted as the tensor describing the curvature of f(W) around W0\n",
    "\n",
    "For this reason, in much the same way that, for a function f(x), you could lower the value of f(x) by moving x by a little bit in the direction opposite to the derivative, with a function f(W) of a tensor, you can lower f(W) by moving W in the direction opposite to the gradient, \n",
    "\n",
    "\n",
    "e.g. ```W1 = W0 - step * gradient(f)(W0)``` \n",
    "(where step is a small scaling factor). \n",
    "\n",
    "\n",
    "That simply means \"going opposite to the curvature\", which intuitively should get you lower on the curve. Note that the scaling factor step is needed because gradient(f)(W0) only approximates the curvature when you are close to W0, so you don’t want to get too far away from W0\n",
    "\n",
    "![title](./pics/sgd.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

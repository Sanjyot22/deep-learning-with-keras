{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using pre-trained convnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./pics/conv_pre_training.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import sys\n",
    "import os, shutil\n",
    "import numpy as np\n",
    "\n",
    "# keras imports \n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "from keras.datasets import mnist\n",
    "from keras.applications import VGG16\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# utility functions\n",
    "import os\n",
    "import sys\n",
    "%matplotlib inline\n",
    "os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(\"__file__\"))))\n",
    "from utility.utils import utils\n",
    "utility_obj = utils()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiating the VGG16 convolutional base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base = VGG16(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(150, 150, 3))\n",
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting features using the pre-trained convolutional base\n",
    "\n",
    "The cats vs. dogs dataset that we will use isn’t packaged with Keras. It was made available by Kaggle.com as part of a computer vision competition in late 2013, back when convnets weren’t quite mainstream. You can download the original dataset at: www.kaggle.com/c/dogs-vs-cats/data (you will need to create a Kaggle account if you don’t already have one—don’t worry, the process is painless)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path to the directory where the original\n",
    "# dataset was uncompressed\n",
    "base_dir = '/home/sj-ai-lsb/Documents/datasets/dogs-vs-cats'\n",
    "\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "test_dir = os.path.join(base_dir, 'test1')\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "batch_size = 20\n",
    "def extract_features(directory, sample_count):\n",
    "    features = np.zeros(shape=(sample_count, 4, 4, 512))\n",
    "    labels = np.zeros(shape=(sample_count))\n",
    "    generator = datagen.flow_from_directory(\n",
    "        directory,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "    i =0\n",
    "    for inputs_batch, labels_batch in generator:\n",
    "            features_batch = conv_base.predict(inputs_batch)\n",
    "            features[i * batch_size : (i + 1) * batch_size] = features_batch\n",
    "            labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
    "            i += 1\n",
    "            if i * batch_size >= sample_count:\n",
    "                # Note that since generators yield data indefinitely in a loop,\n",
    "                # we must `break` after every image has been seen once.\n",
    "                break\n",
    "    return features, labels\n",
    "train_features, train_labels = extract_features(train_dir, 2000)\n",
    "validation_features, validation_labels = extract_features(validation_dir, 1000)\n",
    "test_features, test_labels = extract_features(test_dir, 1000)\n",
    "train_features = np.reshape(train_features, (2000, 4 * 4 * 512))\n",
    "validation_features = np.reshape(validation_features, (1000, 4 * 4 * 512))\n",
    "test_features = np.reshape(test_features, (1000, 4 * 4 * 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the feature vectors to train dense model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model architecture and training\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(256, activation='relu', input_dim=4 * 4 * 512))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=2e-5),\n",
    "              loss='binary_crossentropy',metrics=['acc'])\n",
    "history = model.fit(train_features, train_labels,\n",
    "                    epochs=30, verbose=0,\n",
    "                    batch_size=64,\n",
    "                    validation_data=(validation_features, validation_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ploting model training results\n",
    "utility_obj.plot_training_history(history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a densely-connected classifier on top of the convolutional base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(conv_base)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freezing the convolutional base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('This is the number of trainable weights before freezing the conv base:', len(model.trainable_weights))\n",
    "conv_base.trainable = False\n",
    "print('This is the number of trainable weights after freezing the conv base:', len(model.trainable_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model end-to-end with a frozen convolutional base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=40,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    "\n",
    "# Note that the validation data should not be augmented!\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        # This is the target directory\n",
    "        train_dir,\n",
    "        # All images will be resized to 150x150\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        # Since we use binary_crossentropy loss, we need binary labels\n",
    "        class_mode='binary')\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        class_mode='binary')\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=2e-5),\n",
    "              metrics=['acc'])\n",
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=100,\n",
    "      epochs=30, verbose=0,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ploting model training results\n",
    "utility_obj.plot_training_history(history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freezing all layers up to a specific one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base.trainable = True\n",
    "set_trainable = False\n",
    "for layer in conv_base.layers:\n",
    "    if layer.name == 'block5_conv1':\n",
    "        set_trainable = True\n",
    "    if set_trainable:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "100/100 [==============================] - 13s 128ms/step - loss: 0.2787 - acc: 0.8815 - val_loss: 0.2295 - val_acc: 0.9140\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 12s 116ms/step - loss: 0.2782 - acc: 0.8835 - val_loss: 0.2443 - val_acc: 0.9100\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 11s 115ms/step - loss: 0.2451 - acc: 0.8965 - val_loss: 0.2048 - val_acc: 0.9210\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 11s 114ms/step - loss: 0.2140 - acc: 0.9100 - val_loss: 0.1936 - val_acc: 0.9260\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 12s 117ms/step - loss: 0.2251 - acc: 0.9095 - val_loss: 0.2170 - val_acc: 0.9210\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 12s 119ms/step - loss: 0.2017 - acc: 0.9125 - val_loss: 0.2028 - val_acc: 0.9220\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 12s 116ms/step - loss: 0.1751 - acc: 0.9295 - val_loss: 0.2185 - val_acc: 0.9160\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 12s 119ms/step - loss: 0.1683 - acc: 0.9240 - val_loss: 0.2040 - val_acc: 0.9270\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 12s 119ms/step - loss: 0.1633 - acc: 0.9345 - val_loss: 0.1909 - val_acc: 0.9320\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 12s 117ms/step - loss: 0.1631 - acc: 0.9315 - val_loss: 0.1991 - val_acc: 0.9280\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 12s 118ms/step - loss: 0.1506 - acc: 0.9405 - val_loss: 0.1784 - val_acc: 0.9390\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 12s 118ms/step - loss: 0.1552 - acc: 0.9420 - val_loss: 0.1779 - val_acc: 0.9340\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 12s 117ms/step - loss: 0.1278 - acc: 0.9465 - val_loss: 0.1926 - val_acc: 0.9260\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 12s 115ms/step - loss: 0.1417 - acc: 0.9420 - val_loss: 0.1857 - val_acc: 0.9300\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 11s 114ms/step - loss: 0.1231 - acc: 0.9470 - val_loss: 0.3014 - val_acc: 0.9000\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 11s 115ms/step - loss: 0.1161 - acc: 0.9540 - val_loss: 0.1899 - val_acc: 0.9290\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 12s 118ms/step - loss: 0.1071 - acc: 0.9600 - val_loss: 0.1701 - val_acc: 0.9410\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - 12s 117ms/step - loss: 0.1115 - acc: 0.9560 - val_loss: 0.2441 - val_acc: 0.9160\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 11s 115ms/step - loss: 0.0993 - acc: 0.9625 - val_loss: 0.2377 - val_acc: 0.9150\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 11s 114ms/step - loss: 0.0925 - acc: 0.9600 - val_loss: 0.1971 - val_acc: 0.9300\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 12s 117ms/step - loss: 0.0970 - acc: 0.9600 - val_loss: 0.1998 - val_acc: 0.9260\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 12s 118ms/step - loss: 0.0877 - acc: 0.9685 - val_loss: 0.1639 - val_acc: 0.9350\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - 12s 116ms/step - loss: 0.0955 - acc: 0.9630 - val_loss: 0.2011 - val_acc: 0.9280\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - 12s 117ms/step - loss: 0.0838 - acc: 0.9685 - val_loss: 0.2001 - val_acc: 0.9250\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - 12s 116ms/step - loss: 0.0901 - acc: 0.9660 - val_loss: 0.2020 - val_acc: 0.9280\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - 11s 115ms/step - loss: 0.0693 - acc: 0.9730 - val_loss: 0.2217 - val_acc: 0.9210\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - 12s 116ms/step - loss: 0.0715 - acc: 0.9720 - val_loss: 0.2120 - val_acc: 0.9360\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - 12s 115ms/step - loss: 0.0698 - acc: 0.9735 - val_loss: 0.1835 - val_acc: 0.9380\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - 11s 115ms/step - loss: 0.0797 - acc: 0.9670 - val_loss: 0.2001 - val_acc: 0.9310\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - 12s 116ms/step - loss: 0.0747 - acc: 0.9770 - val_loss: 0.1972 - val_acc: 0.9250\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - 12s 117ms/step - loss: 0.0608 - acc: 0.9735 - val_loss: 0.2180 - val_acc: 0.9310\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - 11s 115ms/step - loss: 0.0540 - acc: 0.9785 - val_loss: 0.2928 - val_acc: 0.9250\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - 11s 114ms/step - loss: 0.0732 - acc: 0.9705 - val_loss: 0.2196 - val_acc: 0.9290\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - 12s 115ms/step - loss: 0.0689 - acc: 0.9745 - val_loss: 0.2245 - val_acc: 0.9320\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - 12s 116ms/step - loss: 0.0529 - acc: 0.9815 - val_loss: 0.2261 - val_acc: 0.9280\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - 12s 115ms/step - loss: 0.0647 - acc: 0.9775 - val_loss: 0.1982 - val_acc: 0.9350\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - 12s 116ms/step - loss: 0.0528 - acc: 0.9810 - val_loss: 0.2292 - val_acc: 0.9330\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - 12s 116ms/step - loss: 0.0440 - acc: 0.9785 - val_loss: 0.2144 - val_acc: 0.9380\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - 12s 117ms/step - loss: 0.0491 - acc: 0.9780 - val_loss: 0.1898 - val_acc: 0.9410\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - 12s 118ms/step - loss: 0.0508 - acc: 0.9810 - val_loss: 0.3125 - val_acc: 0.9200\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - 11s 115ms/step - loss: 0.0526 - acc: 0.9815 - val_loss: 0.2536 - val_acc: 0.9260\n",
      "Epoch 42/100\n",
      "100/100 [==============================] - 11s 115ms/step - loss: 0.0473 - acc: 0.9840 - val_loss: 0.2529 - val_acc: 0.9260\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - 12s 118ms/step - loss: 0.0620 - acc: 0.9765 - val_loss: 0.2163 - val_acc: 0.9320\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - 12s 117ms/step - loss: 0.0598 - acc: 0.9760 - val_loss: 0.2250 - val_acc: 0.9300\n",
      "Epoch 45/100\n",
      "100/100 [==============================] - 12s 115ms/step - loss: 0.0475 - acc: 0.9840 - val_loss: 0.2298 - val_acc: 0.9380\n",
      "Epoch 46/100\n",
      "100/100 [==============================] - 11s 114ms/step - loss: 0.0452 - acc: 0.9830 - val_loss: 0.2294 - val_acc: 0.9350\n",
      "Epoch 47/100\n",
      "100/100 [==============================] - 11s 115ms/step - loss: 0.0339 - acc: 0.9870 - val_loss: 0.2834 - val_acc: 0.9280\n",
      "Epoch 48/100\n",
      "100/100 [==============================] - 12s 116ms/step - loss: 0.0470 - acc: 0.9825 - val_loss: 0.2173 - val_acc: 0.9310\n",
      "Epoch 49/100\n",
      "100/100 [==============================] - 12s 115ms/step - loss: 0.0328 - acc: 0.9860 - val_loss: 0.2336 - val_acc: 0.9300\n",
      "Epoch 50/100\n",
      "100/100 [==============================] - 11s 114ms/step - loss: 0.0462 - acc: 0.9800 - val_loss: 0.2602 - val_acc: 0.9330\n",
      "Epoch 51/100\n",
      "100/100 [==============================] - 12s 119ms/step - loss: 0.0334 - acc: 0.9880 - val_loss: 0.2759 - val_acc: 0.9330\n",
      "Epoch 52/100\n",
      "100/100 [==============================] - 11s 114ms/step - loss: 0.0366 - acc: 0.9880 - val_loss: 0.2565 - val_acc: 0.9310\n",
      "Epoch 53/100\n",
      "100/100 [==============================] - 12s 117ms/step - loss: 0.0377 - acc: 0.9880 - val_loss: 0.2720 - val_acc: 0.9180\n",
      "Epoch 54/100\n",
      "100/100 [==============================] - 11s 114ms/step - loss: 0.0412 - acc: 0.9855 - val_loss: 0.3652 - val_acc: 0.9180\n",
      "Epoch 55/100\n",
      "100/100 [==============================] - 12s 119ms/step - loss: 0.0386 - acc: 0.9840 - val_loss: 0.2541 - val_acc: 0.9310\n",
      "Epoch 56/100\n",
      "100/100 [==============================] - 11s 115ms/step - loss: 0.0374 - acc: 0.9880 - val_loss: 0.2734 - val_acc: 0.9290\n",
      "Epoch 57/100\n",
      "100/100 [==============================] - 12s 116ms/step - loss: 0.0291 - acc: 0.9910 - val_loss: 0.2224 - val_acc: 0.9340\n",
      "Epoch 58/100\n",
      "100/100 [==============================] - 11s 114ms/step - loss: 0.0286 - acc: 0.9895 - val_loss: 0.2616 - val_acc: 0.9320\n",
      "Epoch 59/100\n",
      "100/100 [==============================] - 12s 119ms/step - loss: 0.0277 - acc: 0.9870 - val_loss: 0.3068 - val_acc: 0.9290\n",
      "Epoch 60/100\n",
      "100/100 [==============================] - 11s 114ms/step - loss: 0.0378 - acc: 0.9865 - val_loss: 0.5097 - val_acc: 0.9110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/100\n",
      "100/100 [==============================] - 11s 114ms/step - loss: 0.0458 - acc: 0.9835 - val_loss: 0.3118 - val_acc: 0.9270\n",
      "Epoch 62/100\n",
      "100/100 [==============================] - 12s 115ms/step - loss: 0.0307 - acc: 0.9875 - val_loss: 0.3288 - val_acc: 0.9220\n",
      "Epoch 63/100\n",
      "100/100 [==============================] - 12s 116ms/step - loss: 0.0279 - acc: 0.9895 - val_loss: 0.2248 - val_acc: 0.9370\n",
      "Epoch 64/100\n",
      "100/100 [==============================] - 12s 115ms/step - loss: 0.0265 - acc: 0.9905 - val_loss: 0.2447 - val_acc: 0.9360\n",
      "Epoch 65/100\n",
      "100/100 [==============================] - 11s 114ms/step - loss: 0.0244 - acc: 0.9905 - val_loss: 0.2936 - val_acc: 0.9270\n",
      "Epoch 66/100\n",
      "100/100 [==============================] - 12s 117ms/step - loss: 0.0321 - acc: 0.9875 - val_loss: 0.2724 - val_acc: 0.9300\n",
      "Epoch 67/100\n",
      "100/100 [==============================] - 11s 114ms/step - loss: 0.0290 - acc: 0.9880 - val_loss: 0.2582 - val_acc: 0.9280\n",
      "Epoch 68/100\n",
      "100/100 [==============================] - 12s 116ms/step - loss: 0.0250 - acc: 0.9905 - val_loss: 0.2794 - val_acc: 0.9260\n",
      "Epoch 69/100\n",
      "100/100 [==============================] - 11s 114ms/step - loss: 0.0215 - acc: 0.9925 - val_loss: 0.2880 - val_acc: 0.9340\n",
      "Epoch 70/100\n",
      "100/100 [==============================] - 12s 116ms/step - loss: 0.0372 - acc: 0.9850 - val_loss: 0.2670 - val_acc: 0.9290\n",
      "Epoch 71/100\n",
      "100/100 [==============================] - 12s 115ms/step - loss: 0.0236 - acc: 0.9915 - val_loss: 0.5302 - val_acc: 0.9060\n",
      "Epoch 72/100\n",
      "100/100 [==============================] - 11s 115ms/step - loss: 0.0329 - acc: 0.9885 - val_loss: 0.3096 - val_acc: 0.9290\n",
      "Epoch 73/100\n",
      "100/100 [==============================] - 12s 115ms/step - loss: 0.0256 - acc: 0.9895 - val_loss: 0.2905 - val_acc: 0.9310\n",
      "Epoch 74/100\n",
      "100/100 [==============================] - 12s 115ms/step - loss: 0.0361 - acc: 0.9900 - val_loss: 0.2523 - val_acc: 0.9320\n",
      "Epoch 75/100\n",
      "100/100 [==============================] - 11s 113ms/step - loss: 0.0222 - acc: 0.9915 - val_loss: 0.2630 - val_acc: 0.9370\n",
      "Epoch 76/100\n",
      "100/100 [==============================] - 12s 115ms/step - loss: 0.0298 - acc: 0.9890 - val_loss: 0.2323 - val_acc: 0.9350\n",
      "Epoch 77/100\n",
      "100/100 [==============================] - 12s 115ms/step - loss: 0.0211 - acc: 0.9915 - val_loss: 0.3700 - val_acc: 0.9210\n",
      "Epoch 78/100\n",
      "100/100 [==============================] - 12s 119ms/step - loss: 0.0265 - acc: 0.9900 - val_loss: 0.3065 - val_acc: 0.9250\n",
      "Epoch 79/100\n",
      "100/100 [==============================] - 11s 113ms/step - loss: 0.0237 - acc: 0.9895 - val_loss: 0.2399 - val_acc: 0.9380\n",
      "Epoch 80/100\n",
      "100/100 [==============================] - 11s 115ms/step - loss: 0.0241 - acc: 0.9925 - val_loss: 0.2470 - val_acc: 0.9430\n",
      "Epoch 81/100\n",
      "100/100 [==============================] - 11s 113ms/step - loss: 0.0226 - acc: 0.9940 - val_loss: 0.2529 - val_acc: 0.9400\n",
      "Epoch 82/100\n",
      "100/100 [==============================] - 11s 115ms/step - loss: 0.0310 - acc: 0.9905 - val_loss: 0.2808 - val_acc: 0.9400\n",
      "Epoch 83/100\n",
      "100/100 [==============================] - 12s 115ms/step - loss: 0.0190 - acc: 0.9930 - val_loss: 0.3943 - val_acc: 0.9220\n",
      "Epoch 84/100\n",
      "100/100 [==============================] - 11s 114ms/step - loss: 0.0186 - acc: 0.9920 - val_loss: 0.3464 - val_acc: 0.9340\n",
      "Epoch 85/100\n",
      "100/100 [==============================] - 11s 114ms/step - loss: 0.0235 - acc: 0.9925 - val_loss: 0.3212 - val_acc: 0.9230\n",
      "Epoch 86/100\n",
      "100/100 [==============================] - 11s 113ms/step - loss: 0.0266 - acc: 0.9895 - val_loss: 0.3070 - val_acc: 0.9370\n",
      "Epoch 87/100\n",
      "100/100 [==============================] - 11s 114ms/step - loss: 0.0200 - acc: 0.9940 - val_loss: 0.2988 - val_acc: 0.9320\n",
      "Epoch 88/100\n",
      "100/100 [==============================] - 12s 116ms/step - loss: 0.0283 - acc: 0.9920 - val_loss: 0.2835 - val_acc: 0.9310\n",
      "Epoch 89/100\n",
      "100/100 [==============================] - 11s 115ms/step - loss: 0.0164 - acc: 0.9945 - val_loss: 0.3271 - val_acc: 0.9340\n",
      "Epoch 90/100\n",
      "100/100 [==============================] - 12s 117ms/step - loss: 0.0186 - acc: 0.9945 - val_loss: 0.3191 - val_acc: 0.9270\n",
      "Epoch 91/100\n",
      "100/100 [==============================] - 12s 117ms/step - loss: 0.0196 - acc: 0.9935 - val_loss: 0.2990 - val_acc: 0.9380\n",
      "Epoch 92/100\n",
      "100/100 [==============================] - 12s 118ms/step - loss: 0.0184 - acc: 0.9950 - val_loss: 0.2739 - val_acc: 0.9380\n",
      "Epoch 93/100\n",
      "100/100 [==============================] - 12s 118ms/step - loss: 0.0185 - acc: 0.9935 - val_loss: 0.2848 - val_acc: 0.9400\n",
      "Epoch 94/100\n",
      "100/100 [==============================] - 12s 117ms/step - loss: 0.0208 - acc: 0.9900 - val_loss: 0.2306 - val_acc: 0.9450\n",
      "Epoch 95/100\n",
      "100/100 [==============================] - 12s 117ms/step - loss: 0.0180 - acc: 0.9950 - val_loss: 0.3074 - val_acc: 0.9320\n",
      "Epoch 96/100\n",
      "100/100 [==============================] - 11s 115ms/step - loss: 0.0191 - acc: 0.9935 - val_loss: 0.3492 - val_acc: 0.9310\n",
      "Epoch 97/100\n",
      "100/100 [==============================] - 11s 115ms/step - loss: 0.0208 - acc: 0.9930 - val_loss: 0.5140 - val_acc: 0.9170\n",
      "Epoch 98/100\n",
      "100/100 [==============================] - 12s 116ms/step - loss: 0.0142 - acc: 0.9940 - val_loss: 0.2850 - val_acc: 0.9410\n",
      "Epoch 99/100\n",
      "100/100 [==============================] - 12s 115ms/step - loss: 0.0193 - acc: 0.9915 - val_loss: 0.3733 - val_acc: 0.9310\n",
      "Epoch 100/100\n",
      "100/100 [==============================] - 12s 118ms/step - loss: 0.0259 - acc: 0.9925 - val_loss: 0.2827 - val_acc: 0.9410\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-5),\n",
    "              metrics=['acc'])\n",
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=100,\n",
    "      epochs=100, verbose = 0,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ploting model training results\n",
    "utility_obj.plot_training_history(history.history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## text data encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from IPython.display import display, Markdown #just to display markdown\n",
    "\n",
    "# keras imports\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense\n",
    "\n",
    "# visualiaztion imports\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# general imports\n",
    "import numpy as np  #for vector operation\n",
    "import string   # provides strings variations for character embedding\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## <font color='green'> samples texts </font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['The cat sat on the mat', ' The dog ate my homework']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(Markdown(\"## <font color='green'> samples texts </font>\"))\n",
    "\n",
    "#creating sample text\n",
    "samples = [\"The cat sat on the mat\", \" The dog ate my homework\"] \n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## <font color='green'> word level one hot encoding </font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting to create token_index for every distinct vocabulary\n",
      "token_index created.\n",
      "\n",
      "\n",
      "Starting to create sentence vector using token_index\n",
      "sentences vectorised.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### created word index's for sentence encoding"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The': 1, 'cat': 2, 'sat': 3, 'on': 4, 'the': 5, 'mat': 6, 'dog': 7, 'ate': 8, 'my': 9, 'homework': 10}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### result"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape (2, 11)\n",
      "# sample:2,  specified max length:2,  # vocabulary:11\n",
      "\n",
      "[[0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1.]]\n",
      "\n",
      "Note:  In the final result, index of each element in the vectors denotes the eoncoding of the words.\n",
      "       \"0\"/\"1\" denote presence of a vocabulary\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display(Markdown(\"## <font color='green'> word level one hot encoding </font>\"))\n",
    "\n",
    "#initializing word-encoding dictionary\n",
    "token_index = {} \n",
    "\n",
    "def word_one_hot_embeddings(text_samples,max_sentence_length=8):\n",
    "    \"\"\"\n",
    "        # Creating sentence vectors by represent each word as a vector\n",
    "        # word as a vector: vector of length of vocabulary, with \"1\" for that specified word and zero elsewhere\n",
    "    \"\"\"\n",
    "\n",
    "    # Encoding words in the corpus\n",
    "    print(\"\\nStarting to create token_index for every distinct vocabulary\")\n",
    "    for sample in text_samples:\n",
    "        for word in sample.split():\n",
    "            if word not in token_index.keys():\n",
    "                token_index[word] = len(token_index) + 1\n",
    "    print(\"token_index created.\\n\")\n",
    "\n",
    "    # Word level one hot encoding\n",
    "    print(\"\\nStarting to create sentence vector using token_index\")\n",
    "    results = np.zeros(shape=(len(text_samples),len(token_index)+1))\n",
    "    for i , sample in enumerate(text_samples):\n",
    "        for j , word in enumerate(sample.split()):\n",
    "            index_ = token_index[word]\n",
    "            results[i,index_] = 1\n",
    "    print(\"sentences vectorised.\\n\")\n",
    "    return (results)\n",
    "\n",
    "word_level_vectorised_sample = word_one_hot_embeddings(text_samples=samples,max_sentence_length=8)\n",
    "shape = (word_level_vectorised_sample.shape)\n",
    "\n",
    "\n",
    "\n",
    "display(Markdown(\"### created word index's for sentence encoding\"))\n",
    "print (token_index)\n",
    "\n",
    "display(Markdown(\"### result\"))\n",
    "print(\"Output shape {}\".format(word_level_vectorised_sample.shape))\n",
    "print(\"# sample:{},  specified max length:{},  # vocabulary:{}\\n\".format(len(samples),shape[0],shape[1]))\n",
    "print(word_level_vectorised_sample)\n",
    "\n",
    "print(\n",
    "\"\"\"\n",
    "Note:  In the final result, index of each element in the vectors denotes the eoncoding of the words.\n",
    "       \"0\"/\"1\" denote presence of a vocabulary\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## <font color='green'> word level one hot encoding - KERAS </font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### sentence encoding results"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3, 4, 1, 5], [1, 6, 7, 8, 9]]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### created word index's for sentence encoding"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 1, 'cat': 2, 'sat': 3, 'on': 4, 'mat': 5, 'dog': 6, 'ate': 7, 'my': 8, 'homework': 9}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### result"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape (2, 10)\n",
      "# sample:2,  specified max length:2,  # vocabulary:10\n",
      "\n",
      "[[0. 1. 1. 1. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 1. 1. 1. 1.]]\n",
      "\n",
      "Note:  In the final result, index of each element in the vectors denotes the eoncoding of the words.\n",
      "       \"0\"/\"1\" denote presence of a vocabulary\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display(Markdown(\"## <font color='green'> word level one hot encoding - KERAS </font>\"))\n",
    "# One-hot encoding using keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=len (token_index))\n",
    "tokenizer.fit_on_texts(samples)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(samples) # sentence encoding\n",
    "one_hot_results = tokenizer.texts_to_matrix(samples,mode='binary') # creating one hot results\n",
    "shape = (one_hot_results.shape)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "one_hot_results\n",
    "\n",
    "display(Markdown(\"### sentence encoding results\"))\n",
    "print (sequences)\n",
    "display(Markdown(\"### created word index's for sentence encoding\"))\n",
    "print (word_index)\n",
    "\n",
    "\n",
    "\n",
    "display(Markdown(\"### result\"))\n",
    "print(\"Output shape {}\".format(one_hot_results.shape))\n",
    "print(\"# sample:{},  specified max length:{},  # vocabulary:{}\\n\".format(len(samples),shape[0],shape[1]))\n",
    "print (one_hot_results)\n",
    "\n",
    "print(\n",
    "\"\"\"\n",
    "Note:  In the final result, index of each element in the vectors denotes the eoncoding of the words.\n",
    "       \"0\"/\"1\" denote presence of a vocabulary\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## <font color='green'> character level one hot encoding </font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### characters used for encoding"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 distinct character vocab present\n",
      "Character level indexing\n",
      "{'0': 1, '1': 2, '2': 3, '3': 4, '4': 5, '5': 6, '6': 7, '7': 8, '8': 9, '9': 10, 'a': 11, 'b': 12, 'c': 13, 'd': 14, 'e': 15, 'f': 16, 'g': 17, 'h': 18, 'i': 19, 'j': 20, 'k': 21, 'l': 22, 'm': 23, 'n': 24, 'o': 25, 'p': 26, 'q': 27, 'r': 28, 's': 29, 't': 30, 'u': 31, 'v': 32, 'w': 33, 'x': 34, 'y': 35, 'z': 36, 'A': 37, 'B': 38, 'C': 39, 'D': 40, 'E': 41, 'F': 42, 'G': 43, 'H': 44, 'I': 45, 'J': 46, 'K': 47, 'L': 48, 'M': 49, 'N': 50, 'O': 51, 'P': 52, 'Q': 53, 'R': 54, 'S': 55, 'T': 56, 'U': 57, 'V': 58, 'W': 59, 'X': 60, 'Y': 61, 'Z': 62, '!': 63, '\"': 64, '#': 65, '$': 66, '%': 67, '&': 68, \"'\": 69, '(': 70, ')': 71, '*': 72, '+': 73, ',': 74, '-': 75, '.': 76, '/': 77, ':': 78, ';': 79, '<': 80, '=': 81, '>': 82, '?': 83, '@': 84, '[': 85, '\\\\': 86, ']': 87, '^': 88, '_': 89, '`': 90, '{': 91, '|': 92, '}': 93, '~': 94, ' ': 95, '\\t': 96, '\\n': 97, '\\r': 98, '\\x0b': 99, '\\x0c': 100} \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### result"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape (2, 30, 101)\n",
      "# sample:2,  specified max length:30,  # vocabulary:101\n",
      "\n",
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "display(Markdown(\"## <font color='green'> character level one hot encoding </font>\"))\n",
    "\n",
    "def character_one_hot_embeddings(characters,max_sentence_length_in_characters=30):\n",
    "    \"\"\"\n",
    "        # Creating sentence vectors by using character level encoding\n",
    "    \"\"\"\n",
    "    character_index =  dict(zip(characters,(range(1,len(characters)+1))))\n",
    "    results = np.zeros(shape=(len(samples),max_sentence_length_in_characters,len(characters)+1))\n",
    "\n",
    "    for i , sample in enumerate(samples):\n",
    "        #print (sample)\n",
    "        for j , character in enumerate(sample):\n",
    "            #print (i,j,character_index.get(character),character)\n",
    "            results[i,j,character_index.get(character)] = 1\n",
    "    return (results,character_index)\n",
    "            \n",
    "characters = string.printable\n",
    "char_level_vectorised_sample, character_index = character_one_hot_embeddings(characters)\n",
    "shape = (char_level_vectorised_sample.shape)\n",
    "\n",
    "display(Markdown(\"### characters used for encoding\"))\n",
    "print (\"{} distinct character vocab present\".format(len(characters)))\n",
    "print (\"Character level indexing\")\n",
    "print(character_index,\"\\n\")\n",
    "\n",
    "\n",
    "display(Markdown(\"### result\"))\n",
    "print(\"Output shape {}\".format(char_level_vectorised_sample.shape))\n",
    "print(\"# sample:{},  specified max length:{},  # vocabulary:{}\\n\".format(shape[0],shape[1],shape[2]))\n",
    "print(char_level_vectorised_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## <font color='green'> word level one hot encoding using hashing </font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### result"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 0., 0., 0., 1., 0., 0., 1., 0.],\n",
       "       [1., 1., 1., 0., 0., 0., 0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(Markdown(\"## <font color='green'> word level one hot encoding using hashing </font>\"))\n",
    "dimensionality = len(token_index)\n",
    "max_length = 10\n",
    "\n",
    "results = np.zeros((len(samples),dimensionality))\n",
    "for i, sample in enumerate(samples):\n",
    "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
    "        index= abs(hash(word)% dimensionality)\n",
    "        results[i, index] =1\n",
    "        \n",
    "display(Markdown(\"### result\"))\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## <font color='green'>Understanding word embedding with embedding layer</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### imdb dataset preparation"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data\n",
      "\n",
      "Raw text [\"Working with one of the best Shakespeare sources, this film manages to be creditable to it's source, whilst still appealing to a wider audience.<br /><br />Branagh steals the film from under Fishburne's nose, and there's a talented cast on good form.\", 'Well...tremors I, the original started off in 1990 and i found the movie quite enjoyable to watch. however, they proceeded to make tremors II and III. Trust me, those movies started going downhill right after they finished the first one, i mean, ass blasters??? Now, only God himself is capable of answering the question \"why in Gods name would they create another one of these dumpster dives of a movie?\" Tremors IV cannot be considered a bad movie, in fact it cannot be even considered an epitome of a bad movie, for it lives up to more than that. As i attempted to sit though it, i noticed that my eyes started to bleed, and i hoped profusely that the little girl from the ring would crawl through the TV and kill me. did they really think that dressing the people who had stared in the other movies up as though they we\\'re from the wild west would make the movie (with the exact same occurrences) any better? honestly, i would never suggest buying this movie, i mean, there are cheaper ways to find things that burn well.']\n",
      "\n",
      "text labels [0, 0]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### creating word embedding on imdb dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(\"## <font color='green'>Understanding word embedding with embedding layer</font>\"))\n",
    "display(Markdown(\"### imdb dataset preparation\"))\n",
    "\n",
    "# loading the IMDB dataset\n",
    "\n",
    "# download the dataset from \"ai.stanford.edu/~amaas/data/sentiment/\" \n",
    "# extract the dataset \n",
    "\n",
    "imdb_dir = \"./aclImdb\" # actual path to imdb dataset folder\n",
    "train_dir = os.path.join(imdb_dir,\"train\")\n",
    "\n",
    "\n",
    "# collecting text from each file \n",
    "# collecting corresponding text labels\n",
    "labels = []\n",
    "texts = []\n",
    "for label_type in ['neg', 'pos']:\n",
    "    dir_name = os.path.join(train_dir, label_type)\n",
    "    for fname in os.listdir(dir_name):\n",
    "         if fname[-4:] == '.txt':\n",
    "                f = open(os.path.join(dir_name, fname))\n",
    "                texts.append(f.read())\n",
    "                f.close()\n",
    "                if label_type == 'neg':\n",
    "                    labels.append(0)\n",
    "                else:\n",
    "                    labels.append(1)\n",
    "\n",
    "print (\"Raw data\\n\")\n",
    "\n",
    "print (\"Raw text {}\\n\".format(texts[:2]))\n",
    "print (\"text labels {}\\n\".format(labels[:2]))\n",
    "\n",
    "display(Markdown(\"### creating word embedding on imdb dataset\"))\n",
    "\n",
    "\n",
    "# pre-processing initializations\n",
    "n_vocab = 10000 # max number of distinct vocabulary (top 10,000)\n",
    "max_sentence_length = 100  # Cuts a review after 100 words\n",
    "training_samples = 200 # number of training examples\n",
    "validation_samples = 10000 # number of validation examples\n",
    "\n",
    "# pre-processing dataset\n",
    "\n",
    "# starting to create word emdedding\n",
    "\n",
    "# building tokenizer (keras object that holds index_to_word dictionary, n_vocab, etc parameters)\n",
    "tokenizer = Tokenizer(num_words=n_vocab)\n",
    "tokenizer.fit_on_texts(texts) # feeding the text data to tokenizer\n",
    "\n",
    "# creating word vectors using keras tokenizer & cutting each sentence after 100 words\n",
    "sequences = tokenizer.texts_to_sequences(texts) \n",
    "sequences_padded = pad_sequences(sequences,maxlen=max_sentence_length,\n",
    "                                padding='post', truncating='post') # default paddind is \"pre\" in keras\n",
    "word_index = tokenizer.word_index # word_to_index mappind dictionary\n",
    "\n",
    "print (\"Found {} unique token.\\n\".format(len(word_index)) )\n",
    "\n",
    "# training and testind data split\n",
    "indices = np.arange(sequences_padded.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "sequences_padded = sequences_padded[indices]\n",
    "labels = np.asarray(labels)[indices]\n",
    "x_train = sequences_padded[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "x_val = sequences_padded[training_samples: training_samples + validation_samples]\n",
    "y_val = labels[training_samples: training_samples + validation_samples]\n",
    "\n",
    "print (\"After data preprocessing\")\n",
    "print(\" # Training samples {}, # Testing samples {}\\n\".format(x_train.shape[0],x_val.shape[0]))\n",
    "print(\"Word encoded imdb review data\")\n",
    "print(x_train[:2])\n",
    "\n",
    "print(\"\\nImdb review data lables\")\n",
    "print(y_train[:2])\n",
    "print(\n",
    "\"\"\"\n",
    "Note:  \n",
    "0 : \"Negative review\"\n",
    "1 : \"Positive review\"\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_model_training_history(history):\n",
    "    \n",
    "    #collecting all post-training values \n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "    \n",
    "    # ploting losses\n",
    "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure()\n",
    "    \n",
    "    # ploting accuracies\n",
    "    plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"### building a sequential model using embedding layer\"))\n",
    "\n",
    "\n",
    "# pre-training initializations\n",
    "max_sentence_length = max_sentence_length  # max sentence length taken from imdb dataset preparation stage\n",
    "\n",
    "#embedding layer initializations\n",
    "n_vocab_embedding_layer = n_vocab\n",
    "size_of_word_vector = 8\n",
    "\n",
    "## Embedding layer is trainable layer. It is best understood as a dictionary \n",
    "## that maps integer indices (which stands for specific words) to a dense vectors\n",
    "\n",
    "# Creating a 1 layer neural network\n",
    "model = Sequential()\n",
    "model.add(Embedding(n_vocab_embedding_layer,size_of_word_vector,input_length=max_sentence_length)) \n",
    "model.add(Flatten())\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(optimizer=\"rmsprop\",loss=\"binary_crossentropy\",metrics=[\"acc\"])\n",
    "model.summary()\n",
    "\n",
    "# Starting model training\n",
    "history = model.fit(x_train,y_train,\n",
    "                   epochs = 10,\n",
    "                    batch_size = 32,\n",
    "                    validation_data=(x_val,y_val)\n",
    "                   )\n",
    "\n",
    "# ploting model training results\n",
    "display(Markdown(\"### result\"))\n",
    "plot_model_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"## <font color='green'> Using Glove pre-trained embedding for embedding matrix</font>\"))\n",
    "\n",
    "#loading data from glove embedding to dict\n",
    "embeddings_index = {}\n",
    "f = open(\"./pretrained_embedding_data/glove.6B.100d.txt\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found %s word vectors in pretrained glove embedding.' % len(embeddings_index))\n",
    "\n",
    "# preparing glove word-embedding matrix\n",
    "embedding_dim = 100   # word - vector size as taken from glove embedding\n",
    "word_index = word_index # word_index taken from imdb dataset preparation stage\n",
    "n_vocab_embedding_layer = n_vocab # max vacabulory size taken from imdb dataset preparation stage\n",
    "embedding_matrix = np.zeros((n_vocab_embedding_layer, embedding_dim)) # embedding matrix initialization\n",
    "\n",
    "for word, i in word_index.items(): \n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if i < n_vocab_embedding_layer:\n",
    "        if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector #Words not found in embedding index will be all-zeros.\n",
    "\n",
    "display(Markdown(\"### result\"))\n",
    "print(\"shape of created word-embedding layer : {}\\n\".format(embedding_matrix.shape))\n",
    "print(\"sample representation of encoded words using glove pre-trained embedding\")\n",
    "print(embedding_matrix[2:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"### building a sequential model using glove embeding in embedding layer\"))\n",
    "\n",
    "# pre-processing initializations\n",
    "max_sentence_length = max_sentence_length  # max sentence length taken from imdb dataset preparation stage\n",
    "\n",
    "#embedding layer initializations\n",
    "n_vocab_embedding_layer = n_vocab # max number vocabulary taken from imdb dataset preparation stage\n",
    "size_of_word_vector = embedding_dim # taken from  embedding matrix creation stage\n",
    "\n",
    "# creating a 2 layer neural network\n",
    "model = Sequential()\n",
    "model.add(Embedding(n_vocab_embedding_layer, size_of_word_vector, input_length=max_sentence_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu')) # 1 hidden layer with relu activation\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "# freezing the weights of embedding layer so that glove embedding are maintained\n",
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = False\n",
    "\n",
    "# staring model training\n",
    "model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(x_val, y_val))\n",
    "\n",
    "# ploting model training results\n",
    "display(Markdown(\"### result\"))\n",
    "plot_model_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"## <font color='green'> 2 layer network training embedding layer</font>\"))\n",
    "\n",
    "# pre-processing initializations\n",
    "max_sentence_length = max_sentence_length  # max sentence length taken from imdb dataset preparation stage\n",
    "\n",
    "#embedding layer initializations\n",
    "n_vocab_embedding_layer = n_vocab # max number vocabulary taken from imdb dataset preparation stage\n",
    "size_of_word_vector = embedding_dim # taken from  embedding matrix creation stage\n",
    "\n",
    "\n",
    "# creating a 2 layer neural network\n",
    "model = Sequential()\n",
    "model.add(Embedding(n_vocab_embedding_layer, size_of_word_vector, input_length=max_sentence_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu')) # 1 hidden layer with relu activation\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "## NOTE: No layer freezing\n",
    "\n",
    "# staring model training\n",
    "model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(x_val, y_val))\n",
    "\n",
    "# ploting model training results\n",
    "display(Markdown(\"### result\"))\n",
    "plot_model_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## text data encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "from IPython.display import display, Markdown #just to display markdown\n",
    "import numpy as np  #for vector operation\n",
    "import string   # provides strings variations for character embedding\n",
    "from keras.preprocessing.text import Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## samples texts"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['The cat sat on the mat', ' The dog ate my homework']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(Markdown(\"## samples texts\"))\n",
    "\n",
    "#creating sample text\n",
    "samples = [\"The cat sat on the mat\", \" The dog ate my homework\"] \n",
    "\n",
    "#initializing word-encoding dictionary\n",
    "token_index = {} \n",
    "\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## word level one hot encoding"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting to create token_index for every distinct vocabulary\n",
      "token_index created.\n",
      "\n",
      "\n",
      "Starting to create sentence vector using token_index\n",
      "sentences vectorised.\n",
      "\n",
      "Output shape (2, 8, 11)\n",
      "# sample:2,  specified max length:8,  # vocabulary:11\n",
      "[[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "display(Markdown(\"## word level one hot encoding\"))\n",
    "\n",
    "def word_one_hot_embeddings(text_samples,max_sentence_length=8):\n",
    "    \"\"\"\n",
    "        # Creating sentence vectors by represent each word as a vector\n",
    "        # word as a vector: vector of length of vocabulary, with \"1\" for that specified word and zero elsewhere\n",
    "    \"\"\"\n",
    "\n",
    "    # Encoding words in the corpus\n",
    "    print(\"\\nStarting to create token_index for every distinct vocabulary\")\n",
    "    for sample in text_samples:\n",
    "        for word in sample.split():\n",
    "            if word not in token_index.keys():\n",
    "                token_index[word] = len(token_index) + 1\n",
    "    print(\"token_index created.\\n\")\n",
    "\n",
    "    # Word level one hot encoding\n",
    "    print(\"\\nStarting to create sentence vector using token_index\")\n",
    "    results = np.zeros(shape=(len(text_samples),max_sentence_length,len(token_index)+1))\n",
    "    for i , sample in enumerate(text_samples):\n",
    "        for j , word in enumerate(sample.split()):\n",
    "            index_ = token_index[word]\n",
    "            results[i,j,index_] = 1\n",
    "    print(\"sentences vectorised.\\n\")\n",
    "    return (results)\n",
    "\n",
    "word_level_vectorised_sample = word_one_hot_embeddings(text_samples=samples,max_sentence_length=8)\n",
    "shape = (word_level_vectorised_sample.shape)\n",
    "print(\"Output shape {}\".format(word_level_vectorised_sample.shape))\n",
    "print(\"# sample:{},  specified max length:{},  # vocabulary:{}\".format(shape[0],shape[1],shape[2]))\n",
    "print(word_level_vectorised_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## character level one hot encoding"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### characters used for encoding"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 distinct character vocab present\n",
      "0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ \t\n",
      "\r",
      "\u000b",
      "\f",
      "\n",
      "Output shape (2, 50, 101)\n",
      "# sample:2,  specified max length:50,  # vocabulary:101\n",
      "[[[1. 1. 1. ... 1. 1. 1.]\n",
      "  [1. 1. 1. ... 1. 1. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[1. 1. 1. ... 1. 1. 1.]\n",
      "  [1. 1. 1. ... 1. 1. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "display(Markdown(\"## character level one hot encoding\"))\n",
    "\n",
    "def character_one_hot_embeddings(characters):\n",
    "    \"\"\"\n",
    "        # Creating sentence vectors by using character level encoding\n",
    "    \"\"\"\n",
    "    token_index =  dict(zip((range(1,len(characters)+1)),characters))\n",
    "    max_length  = 50\n",
    "\n",
    "    results = np.zeros(shape=(len(samples),max_length,len(characters)+1))\n",
    "\n",
    "    for i , sample in enumerate(samples):\n",
    "        for j , character in enumerate(samples):\n",
    "            results[i,j,token_index.get(character)] = 1\n",
    "    return (results)\n",
    "            \n",
    "characters = string.printable\n",
    "char_level_vectorised_sample = character_one_hot_embeddings(characters)\n",
    "shape = (char_level_vectorised_sample.shape)\n",
    "\n",
    "display(Markdown(\"### characters used for encoding\"))\n",
    "print (\"{} distinct character vocab present\".format(len(characters)))\n",
    "print (characters)\n",
    "\n",
    "print(\"Output shape {}\".format(char_level_vectorised_sample.shape))\n",
    "print(\"# sample:{},  specified max length:{},  # vocabulary:{}\".format(shape[0],shape[1],shape[2]))\n",
    "print(char_level_vectorised_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## encoding - KERAS"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(Markdown(\"## encoding - KERAS\"))\n",
    "# One-hot encoding using keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=10)\n",
    "tokenizer.fit_on_texts(samples)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(samples)\n",
    "one_hot_results = tokenizer.texts_to_matrix(samples,mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "one_hot_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "\n",
    "# keras imports\n",
    "from keras.layers import Embedding, SimpleRNN, Flatten, Dense\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.datasets import imdb\n",
    "import keras.backend as K\n",
    "import keras\n",
    "import os\n",
    "\n",
    "# general imports\n",
    "from IPython.display import display, Markdown #just to display markdown\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## USING IMDB DATA FOR MODEL TRAINING "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "after data preprocessing\n",
      "input_train shape: (25000, 50)\n",
      "input_test shape: (25000, 50) \n",
      "\n",
      "\n",
      "Imdb review 1 sample input data\n",
      "\n",
      "[[2071   56   26  141    6  194 7486   18    4  226   22   21  134  476\n",
      "    26  480    5  144   30 5535   18   51   36   28  224   92   25  104\n",
      "     4  226   65   16   38 1334   88   12   16  283    5   16 4472  113\n",
      "   103   32   15   16 5345   19  178   32]]\n",
      "\n",
      "Imdb review data lables\n",
      "[1 0]\n",
      "\n",
      "Note:  \n",
      "0 : \"Negative review\"\n",
      "1 : \"Positive review\"\n",
      "post_padding in input data helps gragient in LSTM flow better \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display(Markdown(\"## USING IMDB DATA FOR MODEL TRAINING \"))\n",
    "\n",
    "# pre-processing initializations\n",
    "max_features = 10000  # number of words to consider as features\n",
    "maxlen = 50  # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32\n",
    "\n",
    "# data pre-prcessing\n",
    "### IMDB data preparation was extensively cover in \"text_pre-processing_basic model_building\"(earlier module) ###\n",
    "\n",
    "\n",
    "print('Loading data...')\n",
    "(input_train, y_train), (input_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "# cutting sentences to max length of 50\n",
    "input_train = sequence.pad_sequences(input_train, maxlen=maxlen)\n",
    "input_test = sequence.pad_sequences(input_test, maxlen=maxlen)\n",
    "\n",
    "print (\"after data preprocessing\")\n",
    "print('input_train shape:', input_train.shape)\n",
    "print('input_test shape:', input_test.shape, \"\\n\")\n",
    "\n",
    "print(\"\\nImdb review 1 sample input data\\n\")\n",
    "print(input_train[:1]) \n",
    "\n",
    "print(\"\\nImdb review data lables\")\n",
    "print(y_train[:2])\n",
    "print(\n",
    "\"\"\"\n",
    "Note:  \n",
    "0 : \"Negative review\"\n",
    "1 : \"Positive review\"\n",
    "post_padding in input data helps gragient in LSTM flow better \n",
    "\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keras.callbacks.CSVLogger"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.callbacks.ModelCheckpoint\n",
    "keras.callbacks.EarlyStopping\n",
    "keras.callbacks.LearningRateScheduler\n",
    "keras.callbacks.ReduceLROnPlateau\n",
    "keras.callbacks.CSVLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WRITING THE PRE-DEFINED CALLBACKS FOR MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This callback will save all the training logs in a file after every epoch\n",
    "dir_path = \"./\"\n",
    "csv_logger = keras.callbacks.CSVLogger(\n",
    "    # This is the path to the file where logs data will be stored\n",
    "    os.path.join(dir_path,\"training.log\")\n",
    ")\n",
    "\n",
    "\n",
    "# This callback will interrupt training when we have stopped improving\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    # This callback will monitor the validation accuracy of the model\n",
    "    monitor='val_acc', \n",
    "    # Min improvement requirement in val_accuracy\n",
    "    min_delta=0,\n",
    "    # Training will be interrupted when the accuracy\n",
    "    # has stopped improving for *more* than 3 epochs\n",
    "    patience=3,\n",
    "    # Logs the earlystaging activities\n",
    "    verbose=1, \n",
    "    # Automatically decide if loss/acc as to be min/maximized for evaluation\n",
    "    mode='auto',\n",
    "    # Save only the best weights from the current epoch\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_learning_rate = keras.callbacks.ReduceLROnPlateau(\n",
    "    # This callback will monitor the validation loss of the model\n",
    "    monitor='val_loss',\n",
    "    # It will divide the learning by half when it gets triggered\n",
    "    factor=0.5,\n",
    "    # It will get triggered after the validation loss has stopped improving\n",
    "    # for at least 2 epochs\n",
    "    patience=2,\n",
    "    # Logs the ReduceLROnPlateau activities \n",
    "    verbose=1,\n",
    "    # Note that since the callback will be monitor validation loss,\n",
    "    # we need to pass some `validation_data` to our call to `fit`.\n",
    "    \n",
    "    )\n",
    "\n",
    "# This callback will save the current weights after every epoch\n",
    "# The name of weight file contains epoch number, val accuracy\n",
    "file_path=\"./stored_weights/weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "checkpoints = keras.callbacks.ModelCheckpoint(\n",
    "        filepath=file_path,  # Path to the destination model file\n",
    "        # The two arguments below mean that we will not overwrite the\n",
    "        # model file unless `val_loss` has improved, which\n",
    "        # allows us to keep the best model every seen during training.\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WRITING A CUSTOM CALLING FOR TEST EVALUATION DURING TRAINING\n",
    "\n",
    "Here is a simple example of a custom callback, where we save to disk (as Numpy arrays)\n",
    "the activations of every layer of the model at the end of every epoch, computed on the \n",
    "first sample of the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Called at the start of every epoch\n",
    "# -->on_epoch_begin\n",
    "\n",
    "# Called at the end of every epoch\n",
    "# -->on_epoch_end\n",
    "\n",
    "# Called right before processing each batch\n",
    "# -->on_batch_begin\n",
    "\n",
    "# Called right after processing each batch\n",
    "# -->on_batch_end\n",
    "\n",
    "# Called at the start of training\n",
    "# -->on_train_begin\n",
    "\n",
    "# Called at the end of training\n",
    "# -->on_train_end\n",
    "\n",
    "class TestAccuracyCalculator(keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, test_data=None,test_lables=None):\n",
    "        # Passing test data separately\n",
    "        # to test accuracy changes while training\n",
    "        self.test_data = test_data\n",
    "        self.test_lables = test_lables\n",
    "        \n",
    "    def set_model(self, model):\n",
    "        # This method is called by the parent model\n",
    "        # before training, to inform the callback\n",
    "        # of what model will be calling it\n",
    "        self.model = model\n",
    "    \n",
    "    def calculate_binary_accuracy(self,true_lables,pred_lables):\n",
    "        #accuracy calculation\n",
    "        accuracy = ((np.sum(true_lables == pred_lables))/(len(pred_lables)))*100\n",
    "        return round(accuracy,2)\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "    \n",
    "        if (type(self.test_data) !=  type(None)) and (type(self.test_lables) !=  type(None)):\n",
    "            # Using current best weight from this epoch\n",
    "            # to test on provided test data-set\n",
    "            y_pred = self.model.predict(self.test_data)\n",
    "                \n",
    "            # restructuring predicted vector for accuracy calculation\n",
    "            y_pred_ = np.where(y_pred > 0.5, 1, 0)\n",
    "            y_pred_ = y_pred_.flatten()\n",
    "            accuracy = self.calculate_binary_accuracy(self.test_lables,y_pred_)\n",
    "            print (\"Test accuracy after {} epocs is {}% \\n\".format(epoch+1,accuracy))\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "test_accuracy_calculator = TestAccuracyCalculator(test_data=input_test, test_lables=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks are passed to the model fit the `callbacks` argument in `fit`,\n",
    "# which takes a list of callbacks. You can pass any number of callbacks.\n",
    "callbacks_list = [\n",
    "                  test_accuracy_calculator\n",
    "                  csv_logger, early_stopping,\n",
    "                  reduce_learning_rate,checkpoints,\n",
    "                 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## TRAINING MODEL WITH CALLBACKS "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting model training...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_15 (Embedding)     (None, None, 32)          320000    \n",
      "_________________________________________________________________\n",
      "simple_rnn_15 (SimpleRNN)    (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 322,113\n",
      "Trainable params: 322,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 9s 440us/step - loss: 0.5958 - acc: 0.6632 - val_loss: 0.5308 - val_acc: 0.7354\n",
      "Test accuracy after 1 epocs is 74.28% \n",
      "\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 7s 364us/step - loss: 0.3926 - acc: 0.8299 - val_loss: 0.4531 - val_acc: 0.7954\n",
      "Test accuracy after 2 epocs is 80.16% \n",
      "\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 8s 381us/step - loss: 0.2990 - acc: 0.8754 - val_loss: 0.4338 - val_acc: 0.8030\n",
      "Test accuracy after 3 epocs is 80.47% \n",
      "\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 8s 390us/step - loss: 0.2062 - acc: 0.9226 - val_loss: 0.4886 - val_acc: 0.7912\n",
      "Test accuracy after 4 epocs is 79.17% \n",
      "\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 8s 396us/step - loss: 0.1231 - acc: 0.9575 - val_loss: 0.5810 - val_acc: 0.7772\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Test accuracy after 5 epocs is 78.72% \n",
      "\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 8s 407us/step - loss: 0.0491 - acc: 0.9876 - val_loss: 0.6732 - val_acc: 0.7746\n",
      "Restoring model weights from the end of the best epoch\n",
      "Test accuracy after 6 epocs is 80.47% \n",
      "\n",
      "Epoch 00006: early stopping\n"
     ]
    }
   ],
   "source": [
    "display(Markdown(\"## TRAINING MODEL WITH CALLBACKS \"))\n",
    "\n",
    "# model architecture and training\n",
    "print(\"starting model training...\")\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 32))\n",
    "model.add(SimpleRNN(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "print (\"\\n\")\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(input_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=128,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom callback result\n",
    "\n",
    "As we can see custom call back is printing the accuracy on test after every epoch. We can directly save this accuracy in a seperate log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## CSVLogger callback results"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CSVLogger has saved all the accuracies and losses to \"training.log\" \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### results"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      acc      loss  val_acc  val_loss\n",
      "    0  0.71830  0.542800   0.7964  0.452406\n",
      "    1  0.84550  0.364096   0.8132  0.413793\n",
      "    2  0.88630  0.281825   0.7938  0.444432\n",
      "    3  0.93035  0.191171   0.7750  0.591774\n",
      "    4  0.97125  0.098036   0.7716  0.572219\n"
     ]
    }
   ],
   "source": [
    "display(Markdown(\"## CSVLogger callback results\"))\n",
    "log_data = pd.read_csv(\"training.log\")\n",
    "\n",
    "print(\"\"\"\n",
    "CSVLogger has saved all the accuracies and losses to \"training.log\" \n",
    "\"\"\")\n",
    "display(Markdown(\"### results\"))\n",
    "print (log_data.head().to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EarlyStopping callback result\n",
    "\n",
    "Best validation accuracy was achieved during 3rd epoch. After that as per the \"patience=3\" parameter in EarlyStopping definition, the callback waited for \"val_acc\" improvement. Since there was no improvement better than best val_acc, after 6th epoch model training was stopped. \n",
    "\n",
    "Note: In model training we have mentioned number of epochs to be 10. As per earlystopping model training stopped after 6th epoch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReduceLROnPlateau callback result\n",
    "\n",
    "Best validation accuracy was achieved during 3rd epoch. As per the \"patience=2\" parameter in ReduceLROnPlateau definition, the callback waited for \"val_acc\" improvement for 2 more epochs. Then reduced the learning rate to half (factor=0.5) for further iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## ModelCheckpoint callback results"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelCheckpoint has saved all the model weights to \"stored_weights\" folder\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### results"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.DS_Store', 'weights-improvement-01-0.74.hdf5', 'weights-improvement-03-0.80.hdf5', 'weights-improvement-02-0.80.hdf5', 'weights-improvement-02-0.81.hdf5']\n"
     ]
    }
   ],
   "source": [
    "display(Markdown(\"## ModelCheckpoint callback results\"))\n",
    "\n",
    "print(\"\"\"ModelCheckpoint has saved all the model weights to \"stored_weights\" folder\"\"\")\n",
    "display(Markdown(\"### results\"))\n",
    "print (os.listdir(\"stored_weights/\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

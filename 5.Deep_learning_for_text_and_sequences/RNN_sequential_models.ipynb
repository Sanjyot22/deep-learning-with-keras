{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNDERSTANDING SEQUENTIAL MODEL'S RNN, LSTM & BASIC IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "\n",
    "#keras imports\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, SimpleRNN, Flatten, Input, Dense\n",
    "from keras import layers\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "\n",
    "# general imports\n",
    "from IPython.display import display, Markdown #just to display markdown\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# utility functions\n",
    "import os\n",
    "import sys\n",
    "%matplotlib inline\n",
    "os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(\"__file__\"))))\n",
    "from utility.utils import utils\n",
    "utility_obj = utils()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNDERSTANDING RECURRENT NEURAL NETWORKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RNN processes sequences by iterating through the sequence elements and maintaining \n",
      "a \"state\" containing information relative to what they have seen so far. \n",
      "\n",
      "Note: RNN is a loop\n",
      "\n",
      "state_t = 0\n",
      "for input_t in input_sequence:\n",
      "    output_t = activation(dot(W, input_t) + dot(U, state_t) + b)\n",
      "    state_t = output_t\n",
      "    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (\"\"\"\n",
    "RNN processes sequences by iterating through the sequence elements and maintaining \n",
    "a \"state\" containing information relative to what they have seen so far. \n",
    "\n",
    "Note: RNN is a loop\n",
    "\n",
    "state_t = 0\n",
    "for input_t in input_sequence:\n",
    "    output_t = activation(dot(W, input_t) + dot(U, state_t) + b)\n",
    "    state_t = output_t\n",
    "    \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NUMPY IMPLEMENTATION OF RECURRENT NEURAL NETWORKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data\n",
      "\n",
      "Shape of input data: (100, 32)\n",
      "first 2 timestamps data \n",
      "[[0.26476403 0.38664106 0.46044452 0.9420754  0.5353783  0.82272518\n",
      "  0.33681613 0.3776788  0.08062277 0.84122617 0.5615275  0.62928122\n",
      "  0.97167874 0.26011035 0.80086371 0.34228471 0.15533864 0.45641351\n",
      "  0.30320683 0.14014776 0.04380577 0.81472504 0.24983742 0.12155904\n",
      "  0.08964197 0.82487408 0.15654995 0.96798776 0.81131087 0.19076223\n",
      "  0.5361914  0.37642617]\n",
      " [0.68158042 0.28571638 0.72640482 0.62631059 0.83873249 0.68955908\n",
      "  0.98209671 0.64570299 0.20639038 0.02058361 0.65060127 0.33364746\n",
      "  0.33571762 0.60945248 0.94112809 0.99581311 0.93049198 0.66276085\n",
      "  0.35261523 0.99120257 0.3075645  0.15928075 0.88350928 0.61108008\n",
      "  0.60224278 0.34402908 0.81930911 0.73534425 0.66507488 0.58049435\n",
      "  0.20272494 0.81760415]]\n",
      "\n",
      "Expected output shape/timestamp: (64,)\n",
      "Random initial output state \n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### result"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have a 64 dimension vector for each time-step of input sequence\n",
      "\n",
      "[[0.99999966 0.99999997 0.99999975 ... 0.99999991 0.99999998 0.99999983]\n",
      " [1.         1.         1.         ... 1.         1.         1.        ]\n",
      " [1.         1.         1.         ... 1.         1.         1.        ]\n",
      " ...\n",
      " [1.         1.         1.         ... 1.         1.         1.        ]\n",
      " [1.         1.         1.         ... 1.         1.         1.        ]\n",
      " [1.         1.         1.         ... 1.         1.         1.        ]]\n",
      "\n",
      "Note:\n",
      "\n",
      "- RNN in each loop, produce result for that particular timestamp\n",
      "- While in a loop, RNN has output from earlier states stored in the form of state variable\n",
      "- This output corresponds is produced when \"return sequence = True\" in rnn parameter initialization\n",
      "- The above input data is 1 sample in a batch (for next sample the loop restarts)\n",
      "- The above implementation is the actual Keras SimpleRNN layer \n",
      "(the only difference is it can process batches of sequence)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "timesteps = 100  # Number of timesteps in the input sequence\n",
    "inputs_features = 32  # Dimensionality of the input feature space\n",
    "output_features = 64  # Dimensionality of the output feature space\n",
    "\n",
    "\n",
    "# This is our input data - just random noise for the sake of our example.\n",
    "inputs = np.random.random((timesteps, inputs_features))\n",
    "# This is our \"initial state\": an all-zero vector.\n",
    "state_t = np.zeros((output_features,))\n",
    "\n",
    "\n",
    "\n",
    "print (\"Raw data\\n\")\n",
    "print(\"Shape of input data: {}\".format(inputs.shape))\n",
    "print (\"first 2 timestamps data \\n{}\\n\".format(inputs[:2]))\n",
    "print(\"Expected output shape/timestamp: {}\".format(state_t.shape))\n",
    "print (\"Random initial output state \\n{}\\n\".format(state_t))\n",
    "\n",
    "# Creating random weight matrices\n",
    "W = np.random.random((output_features, inputs_features))\n",
    "U = np.random.random((output_features, output_features))\n",
    "b = np.random.random((output_features,))\n",
    "\n",
    "successive_outputs = []\n",
    "for input_t in inputs:  # input_t is a vector of shape (input_features,)\n",
    "    # We combine the input with the current state\n",
    "    # (i.e. the previous output) to obtain the current output.\n",
    "    \n",
    "    output_t = np.tanh(np.dot(W, input_t) + np.dot(U, state_t) + b)\n",
    "    \n",
    "    # We store this output in a list.\n",
    "    successive_outputs.append(output_t)\n",
    "    \n",
    "    # We update the \"state\" of the network for the next timestep\n",
    "    state_t = output_t\n",
    "    \n",
    "\n",
    "\n",
    "display(Markdown(\"### result\"))\n",
    "\n",
    "# The final output is a 2D tensor of shape (timesteps, output_features).\n",
    "# final_output_sequence = np.concatenate(successive_outputs, axis=0)\n",
    "final_output_sequence = np.reshape(successive_outputs, (timesteps,output_features))\n",
    "\n",
    "print (\"We have a {} dimension vector for each time-step of input sequence\\n\".format(final_output_sequence.shape[1]))\n",
    "print (final_output_sequence)\n",
    "\n",
    "print (\"\"\"\n",
    "Note:\n",
    "\n",
    "- RNN in each loop, produce result for that particular timestamp\n",
    "- While in a loop, RNN has output from earlier states stored in the form of state variable\n",
    "- This output corresponds is produced when \"return sequence = True\" in rnn parameter initialization\n",
    "- The above input data is 1 sample in a batch (for next sample the loop restarts)\n",
    "- The above implementation is the actual Keras SimpleRNN layer \n",
    "(the only difference is it can process batches of sequence)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing above model using keras SimpleRNN layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "rnn_layer (SimpleRNN)        (None, 100, 64)           6208      \n",
      "=================================================================\n",
      "Total params: 6,208\n",
      "Trainable params: 6,208\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Note:\n",
      "In case of text, the embedding layer help to create this 2 dimensional input vector. \n",
      "In text each word represent the timestamp and word vector is smallest data unit.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model architecture\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(64,return_sequences=True,input_shape=(100,32),name=\"rnn_layer\"))\n",
    "model.summary()\n",
    "\n",
    "print (\"\"\"\n",
    "Note:\n",
    "In case of text, the embedding layer help to create this 2 dimensional input vector. \n",
    "In text each word represent the timestamp and word vector is smallest data unit.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USING SIMPLE-RNN & RETURNING THE LAST STATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_layer (Embedding)  (None, 100, 32)           320000    \n",
      "_________________________________________________________________\n",
      "rnn_layer (SimpleRNN)        (None, 64)                6208      \n",
      "=================================================================\n",
      "Total params: 326,208\n",
      "Trainable params: 326,208\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### USING SIMPLE-RNN & RETURNING ALL STATE's SEQUENCES"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_layer (Embedding)  (None, 100, 32)           320000    \n",
      "_________________________________________________________________\n",
      "rnn_layer (SimpleRNN)        (None, 100, 64)           6208      \n",
      "=================================================================\n",
      "Total params: 326,208\n",
      "Trainable params: 326,208\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "\"return_sequences\" parameter decides if the output is produced at each stage or final output is produced \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#embedding layer initializations\n",
    "n_vocab_embedding_layer = 10000\n",
    "size_of_word_vector = 32\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(n_vocab_embedding_layer, output_dim=size_of_word_vector , input_length=100,name=\"embedding_layer\"))\n",
    "model.add(SimpleRNN(64, name = \"rnn_layer\"))\n",
    "model.summary()\n",
    "\n",
    "\n",
    "display(Markdown(\"### USING SIMPLE-RNN & RETURNING ALL STATE's SEQUENCES\"))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(n_vocab_embedding_layer, output_dim=size_of_word_vector, input_length=100,name=\"embedding_layer\"))\n",
    "model.add(SimpleRNN(64, return_sequences=True, name = \"rnn_layer\"))\n",
    "model.summary()\n",
    "\n",
    "print (\"\"\"\n",
    "\"return_sequences\" parameter decides if the output is produced at each stage or final output is produced \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOADING DATA FOR ALL THE BELOW MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "after data preprocessing\n",
      "input_train shape: (25000, 500)\n",
      "input_test shape: (25000, 500) \n",
      "\n",
      "\n",
      "Imdb review 1 sample input data\n",
      "\n",
      "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    1   14   22   16   43  530  973 1622 1385   65  458 4468\n",
      "    66 3941    4  173   36  256    5   25  100   43  838  112   50  670\n",
      "     2    9   35  480  284    5  150    4  172  112  167    2  336  385\n",
      "    39    4  172 4536 1111   17  546   38   13  447    4  192   50   16\n",
      "     6  147 2025   19   14   22    4 1920 4613  469    4   22   71   87\n",
      "    12   16   43  530   38   76   15   13 1247    4   22   17  515   17\n",
      "    12   16  626   18    2    5   62  386   12    8  316    8  106    5\n",
      "     4 2223 5244   16  480   66 3785   33    4  130   12   16   38  619\n",
      "     5   25  124   51   36  135   48   25 1415   33    6   22   12  215\n",
      "    28   77   52    5   14  407   16   82    2    8    4  107  117 5952\n",
      "    15  256    4    2    7 3766    5  723   36   71   43  530  476   26\n",
      "   400  317   46    7    4    2 1029   13  104   88    4  381   15  297\n",
      "    98   32 2071   56   26  141    6  194 7486   18    4  226   22   21\n",
      "   134  476   26  480    5  144   30 5535   18   51   36   28  224   92\n",
      "    25  104    4  226   65   16   38 1334   88   12   16  283    5   16\n",
      "  4472  113  103   32   15   16 5345   19  178   32]]\n",
      "\n",
      "Imdb review data lables\n",
      "[1 0]\n",
      "\n",
      "Note:  \n",
      "0 : \"Negative review\"\n",
      "1 : \"Positive review\"\n",
      "post_padding in input data helps gragient in LSTM flow better \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pre-processing initializations\n",
    "max_features = 10000  # number of words to consider as features\n",
    "maxlen = 500  # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32\n",
    "\n",
    "# data pre-prcessing\n",
    "### IMDB data encoding from text will be extensively covered in \"text_pre-processing_basic_model_building\"###\n",
    "\n",
    "\n",
    "print('Loading data...')\n",
    "(input_train, y_train), (input_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "# cutting sentences to max length of 500\n",
    "input_train = sequence.pad_sequences(input_train, maxlen=maxlen)\n",
    "input_test = sequence.pad_sequences(input_test, maxlen=maxlen)\n",
    "\n",
    "print (\"after data preprocessing\")\n",
    "print('input_train shape:', input_train.shape)\n",
    "print('input_test shape:', input_test.shape, \"\\n\")\n",
    "\n",
    "print(\"\\nImdb review 1 sample input data\\n\")\n",
    "print(input_train[:1]) \n",
    "\n",
    "print(\"\\nImdb review data lables\")\n",
    "print(y_train[:2])\n",
    "print(\n",
    "\"\"\"\n",
    "Note:  \n",
    "0 : \"Negative review\"\n",
    "1 : \"Positive review\"\n",
    "post_padding in input data helps gragient in LSTM flow better \n",
    "\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING MODEL USING SIMPLE-RNN LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting model training...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 64)          640000    \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 648,321\n",
      "Trainable params: 648,321\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 21s 1ms/step - loss: 0.5955 - acc: 0.6727 - val_loss: 0.4123 - val_acc: 0.8284\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 20s 987us/step - loss: 0.3760 - acc: 0.8438 - val_loss: 0.4232 - val_acc: 0.8076\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 19s 959us/step - loss: 0.3046 - acc: 0.8754 - val_loss: 0.4386 - val_acc: 0.8254\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 20s 984us/step - loss: 0.2224 - acc: 0.9145 - val_loss: 0.4289 - val_acc: 0.8434\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 20s 991us/step - loss: 0.2151 - acc: 0.9160 - val_loss: 0.6577 - val_acc: 0.7078\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 20s 992us/step - loss: 0.1484 - acc: 0.9434 - val_loss: 0.4759 - val_acc: 0.8196\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 20s 1ms/step - loss: 0.1425 - acc: 0.9508 - val_loss: 0.5109 - val_acc: 0.8370\n",
      "Epoch 8/10\n",
      " 8320/20000 [===========>..................] - ETA: 10s - loss: 0.0567 - acc: 0.9811"
     ]
    }
   ],
   "source": [
    "# model architecture and training\n",
    "print(\"starting model training...\")\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 64))\n",
    "model.add(SimpleRNN(64))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "print (\"\\n\")\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(input_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=128,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ploting model training results\n",
    "utility_obj.plot_training_history(history.history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
